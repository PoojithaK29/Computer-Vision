{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (2.16.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (71.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.12.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\pkathi\\appdata\\roaming\\python\\python310\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow opencv-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''CNN MODEL'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkathi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 142 images belonging to 3 classes.\n",
      "Found 33 images belonging to 3 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkathi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 162ms/step - accuracy: 0.3226 - loss: 0.9203 - val_accuracy: 0.4545 - val_loss: 0.7180\n",
      "Epoch 2/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.7508 - loss: 0.7898 - val_accuracy: 0.4545 - val_loss: 0.7220\n",
      "Epoch 3/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.8449 - loss: 0.5916 - val_accuracy: 0.4545 - val_loss: 0.7580\n",
      "Epoch 4/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.8456 - loss: 0.5728 - val_accuracy: 0.4545 - val_loss: 1.4072\n",
      "Epoch 5/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 0.8476 - loss: 0.4601 - val_accuracy: 0.4545 - val_loss: 1.4851\n",
      "Epoch 6/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.8275 - loss: 0.4559 - val_accuracy: 0.4545 - val_loss: 1.7736\n",
      "Epoch 7/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.8532 - loss: 0.3794 - val_accuracy: 0.4545 - val_loss: 1.9625\n",
      "Epoch 8/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 0.8826 - loss: 0.2903 - val_accuracy: 0.4545 - val_loss: 2.2186\n",
      "Epoch 9/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.8796 - loss: 0.2914 - val_accuracy: 0.4545 - val_loss: 2.2348\n",
      "Epoch 10/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - accuracy: 0.9430 - loss: 0.2447 - val_accuracy: 0.4545 - val_loss: 2.2785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Define a simple CNN model\n",
    "def create_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(3, activation='softmax')  # 3 classes: fire, smoke, sparkle\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Prepare the dataset\n",
    "def prepare_dataset(data_dir, target_size=(64, 64), batch_size=32):\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, train_generator, validation_generator, epochs=10):\n",
    "    model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
    "\n",
    "# Use the model to classify abnormalities\n",
    "def classify_abnormality(model, frame):\n",
    "    frame_resized = cv2.resize(frame, (64, 64))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    classes = ['Fire', 'Smoke', 'Sparkle']\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "# Example usage\n",
    "input_shape = (64, 64, 3)\n",
    "cnn_model = create_cnn_model(input_shape)\n",
    "data_dir = r'C:\\Users\\pkathi\\Desktop\\object detection\\tradianal approach-cv\\Dataset'\n",
    "train_gen, val_gen = prepare_dataset(data_dir)\n",
    "train_model(cnn_model, train_gen, val_gen)\n",
    "\n",
    "# Save the trained model\n",
    "cnn_model.save('abnormality_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Abnormality 1 start time: 0 minutes 45.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 12.00 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Abnormality 1 start time: 0 minutes 57.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Abnormality 1 start time: 0 minutes 57.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Abnormality 1 start time: 0 minutes 58.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Total video time: 0 minutes 60.00 seconds\n",
      "Abnormality detected at frame numbers: [1350, 1722, 1734, 1746, 1758]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def process_frame(frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id, model):\n",
    "    frame_resized = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    frame_diff = cv2.absdiff(prev_frame_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = any(cv2.contourArea(contour) > min_contour_area for contour in contours)\n",
    "    if detected:\n",
    "        abnormality_class = classify_abnormality(model, frame_resized)\n",
    "        output_frame_path = os.path.join(output_dir, f\"abnormality_{abnormality_group_id}_{abnormality_class}\", f\"frame_{frame_number}.jpg\")\n",
    "        os.makedirs(os.path.dirname(output_frame_path), exist_ok=True)\n",
    "        cv2.imwrite(output_frame_path, frame_resized)\n",
    "    \n",
    "    return detected, frame_gray\n",
    "\n",
    "def motion_detection(video_path, output_dir, threshold_value=25, min_contour_area=500, resize_factor=0.5, minimal_fps=5, min_abnormality_gap_minutes=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip_interval = int(round(original_fps / minimal_fps))\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame\")\n",
    "        return\n",
    "\n",
    "    prev_frame = cv2.resize(prev_frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    abnormality_times = []\n",
    "\n",
    "    frame_number = 0\n",
    "    abnormality_detected = False\n",
    "    abnormality_start_frame = None\n",
    "    abnormality_group_id = 0\n",
    "    last_abnormality_time = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pool = ThreadPool(processes=4)  # Use a thread pool with 4 threads\n",
    "\n",
    "    # Load the trained model\n",
    "    model = load_model('abnormality_classifier.h5')\n",
    "\n",
    "    while True:\n",
    "        for _ in range(frame_skip_interval):\n",
    "            ret = cap.grab()  # Use grab to skip frames efficiently\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_number += 1\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.retrieve()  # Retrieve the frame after skipping\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use thread pool to process frame\n",
    "        result = pool.apply_async(process_frame, (frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id, model))\n",
    "        detected, prev_frame_gray = result.get()\n",
    "\n",
    "        if detected:\n",
    "            current_abnormality_time = frame_number / original_fps\n",
    "\n",
    "            if not abnormality_detected:\n",
    "                if last_abnormality_time is None or (current_abnormality_time - last_abnormality_time) >= (min_abnormality_gap_minutes * 60):\n",
    "                    abnormality_group_id += 1\n",
    "                    os.makedirs(os.path.join(output_dir, f\"abnormality_{abnormality_group_id}\"))\n",
    "\n",
    "                abnormality_start_frame = frame_number\n",
    "                abnormality_detected = True\n",
    "                abnormality_times.append(frame_number)\n",
    "                last_abnormality_time = current_abnormality_time\n",
    "\n",
    "        else:\n",
    "            if abnormality_detected:\n",
    "                abnormality_detected = False\n",
    "                abnormality_end_frame = frame_number\n",
    "                abnormality_start_seconds = abnormality_start_frame / original_fps\n",
    "                abnormality_end_seconds = abnormality_end_frame / original_fps\n",
    "                abnormality_duration_seconds = abnormality_end_seconds - abnormality_start_seconds\n",
    "\n",
    "                print(f\"Abnormality {abnormality_group_id} start time: {int(abnormality_start_seconds // 60)} minutes {abnormality_start_seconds % 60:.2f} seconds\")\n",
    "                print(f\"Abnormality {abnormality_group_id} duration: {int(abnormality_duration_seconds // 60)} minutes {abnormality_duration_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    cap.release()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    total_time_seconds = frame_number / original_fps\n",
    "    print(f\"Total video time: {int(total_time_seconds // 60)} minutes {total_time_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    return abnormality_times\n",
    "\n",
    "# Example usage\n",
    "video_path = r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\output48.mp4\"\n",
    "output_dir = 'C:/Users/pkathi/Desktop/my work/computervision-video/cnntrail1'\n",
    "abnormality_times = motion_detection(video_path, output_dir, minimal_fps=5)\n",
    "\n",
    "if abnormality_times:\n",
    "    print(\"Abnormality detected at frame numbers:\", abnormality_times)\n",
    "else:\n",
    "    print(\"No abnormality detected in the video.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Abnormality 1 start time: 0 minutes 57.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.80 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Abnormality 1 start time: 0 minutes 58.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Abnormality 1 start time: 1 minutes 3.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Abnormality 1 start time: 1 minutes 3.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Abnormality 1 start time: 1 minutes 17.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Abnormality 1 start time: 1 minutes 17.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "Total video time: 1 minutes 60.00 seconds\n",
      "Abnormality detected at frame numbers: [1728, 1758, 1890, 1902, 2310, 2322]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def process_frame(frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id, model):\n",
    "    frame_resized = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    frame_diff = cv2.absdiff(prev_frame_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = any(cv2.contourArea(contour) > min_contour_area for contour in contours)\n",
    "    if detected:\n",
    "        abnormality_class = classify_abnormality(model, frame_resized)\n",
    "        output_frame_path = os.path.join(output_dir, f\"abnormality_{abnormality_group_id}_{abnormality_class}\", f\"frame_{frame_number}.jpg\")\n",
    "        os.makedirs(os.path.dirname(output_frame_path), exist_ok=True)\n",
    "        cv2.imwrite(output_frame_path, frame_resized)\n",
    "    \n",
    "    return detected, frame_gray\n",
    "\n",
    "def motion_detection(video_path, output_dir, threshold_value=25, min_contour_area=500, resize_factor=0.5, minimal_fps=5, min_abnormality_gap_minutes=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip_interval = int(round(original_fps / minimal_fps))\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame\")\n",
    "        return\n",
    "\n",
    "    prev_frame = cv2.resize(prev_frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    abnormality_times = []\n",
    "\n",
    "    frame_number = 0\n",
    "    abnormality_detected = False\n",
    "    abnormality_start_frame = None\n",
    "    abnormality_group_id = 0\n",
    "    last_abnormality_time = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pool = ThreadPool(processes=4)  # Use a thread pool with 4 threads\n",
    "\n",
    "    # Load the trained model\n",
    "    model = load_model('abnormality_classifier.h5')\n",
    "\n",
    "    while True:\n",
    "        for _ in range(frame_skip_interval):\n",
    "            ret = cap.grab()  # Use grab to skip frames efficiently\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_number += 1\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.retrieve()  # Retrieve the frame after skipping\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use thread pool to process frame\n",
    "        result = pool.apply_async(process_frame, (frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id, model))\n",
    "        detected, prev_frame_gray = result.get()\n",
    "\n",
    "        if detected:\n",
    "            current_abnormality_time = frame_number / original_fps\n",
    "\n",
    "            if not abnormality_detected:\n",
    "                if last_abnormality_time is None or (current_abnormality_time - last_abnormality_time) >= (min_abnormality_gap_minutes * 60):\n",
    "                    abnormality_group_id += 1\n",
    "                    os.makedirs(os.path.join(output_dir, f\"abnormality_{abnormality_group_id}\"))\n",
    "\n",
    "                abnormality_start_frame = frame_number\n",
    "                abnormality_detected = True\n",
    "                abnormality_times.append(frame_number)\n",
    "                last_abnormality_time = current_abnormality_time\n",
    "\n",
    "        else:\n",
    "            if abnormality_detected:\n",
    "                abnormality_detected = False\n",
    "                abnormality_end_frame = frame_number\n",
    "                abnormality_start_seconds = abnormality_start_frame / original_fps\n",
    "                abnormality_end_seconds = abnormality_end_frame / original_fps\n",
    "                abnormality_duration_seconds = abnormality_end_seconds - abnormality_start_seconds\n",
    "\n",
    "                print(f\"Abnormality {abnormality_group_id} start time: {int(abnormality_start_seconds // 60)} minutes {abnormality_start_seconds % 60:.2f} seconds\")\n",
    "                print(f\"Abnormality {abnormality_group_id} duration: {int(abnormality_duration_seconds // 60)} minutes {abnormality_duration_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    cap.release()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    total_time_seconds = frame_number / original_fps\n",
    "    print(f\"Total video time: {int(total_time_seconds // 60)} minutes {total_time_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    return abnormality_times\n",
    "\n",
    "# Example usage\n",
    "video_path = r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\C1007trimmed116-118.mp4\"\n",
    "output_dir = 'C:/Users/pkathi/Desktop/my work/computervision-video/cnntrail2'\n",
    "abnormality_times = motion_detection(video_path, output_dir, minimal_fps=5)\n",
    "\n",
    "if abnormality_times:\n",
    "    print(\"Abnormality detected at frame numbers:\", abnormality_times)\n",
    "else:\n",
    "    print(\"No abnormality detected in the video.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''EfficientNetB0 as the base model for transfer learning, which is a state-of-the-art convolutional neural network (CNN) architecture that achieves a good balance between performance and computational efficiency. EfficientNet is a popular choice for image classification tasks, including detecting objects like fire and smoke.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1767 images belonging to 3 classes.\n",
      "Found 440 images belonging to 3 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 156ms/step - accuracy: 0.9461 - loss: 0.3155 - val_accuracy: 0.9477 - val_loss: 0.1334\n",
      "Epoch 2/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 142ms/step - accuracy: 0.9761 - loss: 0.0774 - val_accuracy: 0.9568 - val_loss: 0.1184\n",
      "Epoch 3/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 140ms/step - accuracy: 0.9815 - loss: 0.0671 - val_accuracy: 0.9477 - val_loss: 0.1537\n",
      "Epoch 4/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 145ms/step - accuracy: 0.9750 - loss: 0.0906 - val_accuracy: 0.9477 - val_loss: 0.1228\n",
      "Epoch 5/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 141ms/step - accuracy: 0.9775 - loss: 0.0630 - val_accuracy: 0.9591 - val_loss: 0.1373\n",
      "Epoch 6/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 143ms/step - accuracy: 0.9816 - loss: 0.0573 - val_accuracy: 0.9682 - val_loss: 0.1022\n",
      "Epoch 7/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 139ms/step - accuracy: 0.9820 - loss: 0.0536 - val_accuracy: 0.9591 - val_loss: 0.1295\n",
      "Epoch 8/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 140ms/step - accuracy: 0.9844 - loss: 0.0347 - val_accuracy: 0.8068 - val_loss: 0.3745\n",
      "Epoch 9/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 140ms/step - accuracy: 0.9864 - loss: 0.0480 - val_accuracy: 0.9500 - val_loss: 0.1036\n",
      "Epoch 10/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 139ms/step - accuracy: 0.9914 - loss: 0.0370 - val_accuracy: 0.9614 - val_loss: 0.0994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Define a simple CNN model\n",
    "def create_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(3, activation='softmax')  # 3 classes: fire, smoke, sparkle\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Prepare the dataset\n",
    "def prepare_dataset(data_dir, target_size=(64, 64), batch_size=32):\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, train_generator, validation_generator, epochs=10):\n",
    "    model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
    "\n",
    "# Use the model to classify abnormalities\n",
    "def classify_abnormality(model, frame):\n",
    "    frame_resized = cv2.resize(frame, (64, 64))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    classes = ['Fire', 'Smoke', 'Sparkle']\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "# Example usage\n",
    "input_shape = (64, 64, 3)\n",
    "cnn_model = create_cnn_model(input_shape)\n",
    "data_dir = r'C:\\Users\\pkathi\\Desktop\\object detection\\tradianal approach-cv\\Dataset'\n",
    "train_gen, val_gen = prepare_dataset(data_dir)\n",
    "train_model(cnn_model, train_gen, val_gen)\n",
    "\n",
    "# Save the trained model\n",
    "cnn_model.save('abnormality_classifier2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Abnormality 1 start time: 0 minutes 57.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.80 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Abnormality 1 start time: 0 minutes 58.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Abnormality 1 start time: 1 minutes 3.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Abnormality 1 start time: 1 minutes 3.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Abnormality 1 start time: 1 minutes 17.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Abnormality 1 start time: 1 minutes 17.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "Total video time: 1 minutes 60.00 seconds\n",
      "Abnormality detected at frame numbers: [1728, 1758, 1890, 1902, 2310, 2322]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def process_frame(frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id, model):\n",
    "    frame_resized = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    frame_diff = cv2.absdiff(prev_frame_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = any(cv2.contourArea(contour) > min_contour_area for contour in contours)\n",
    "    if detected:\n",
    "        abnormality_class = classify_abnormality(model, frame_resized)\n",
    "        output_frame_path = os.path.join(output_dir, f\"abnormality_{abnormality_group_id}_{abnormality_class}\", f\"frame_{frame_number}.jpg\")\n",
    "        os.makedirs(os.path.dirname(output_frame_path), exist_ok=True)\n",
    "        cv2.imwrite(output_frame_path, frame_resized)\n",
    "    \n",
    "    return detected, frame_gray\n",
    "\n",
    "def motion_detection(video_path, output_dir, threshold_value=25, min_contour_area=500, resize_factor=0.5, minimal_fps=5, min_abnormality_gap_minutes=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip_interval = int(round(original_fps / minimal_fps))\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame\")\n",
    "        return\n",
    "\n",
    "    prev_frame = cv2.resize(prev_frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    abnormality_times = []\n",
    "\n",
    "    frame_number = 0\n",
    "    abnormality_detected = False\n",
    "    abnormality_start_frame = None\n",
    "    abnormality_group_id = 0\n",
    "    last_abnormality_time = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pool = ThreadPool(processes=4)  # Use a thread pool with 4 threads\n",
    "\n",
    "    # Load the trained model\n",
    "    model = load_model('abnormality_classifier2.h5')\n",
    "\n",
    "    while True:\n",
    "        for _ in range(frame_skip_interval):\n",
    "            ret = cap.grab()  # Use grab to skip frames efficiently\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_number += 1\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.retrieve()  # Retrieve the frame after skipping\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use thread pool to process frame\n",
    "        result = pool.apply_async(process_frame, (frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id, model))\n",
    "        detected, prev_frame_gray = result.get()\n",
    "\n",
    "        if detected:\n",
    "            current_abnormality_time = frame_number / original_fps\n",
    "\n",
    "            if not abnormality_detected:\n",
    "                if last_abnormality_time is None or (current_abnormality_time - last_abnormality_time) >= (min_abnormality_gap_minutes * 60):\n",
    "                    abnormality_group_id += 1\n",
    "                    os.makedirs(os.path.join(output_dir, f\"abnormality_{abnormality_group_id}\"))\n",
    "\n",
    "                abnormality_start_frame = frame_number\n",
    "                abnormality_detected = True\n",
    "                abnormality_times.append(frame_number)\n",
    "                last_abnormality_time = current_abnormality_time\n",
    "\n",
    "        else:\n",
    "            if abnormality_detected:\n",
    "                abnormality_detected = False\n",
    "                abnormality_end_frame = frame_number\n",
    "                abnormality_start_seconds = abnormality_start_frame / original_fps\n",
    "                abnormality_end_seconds = abnormality_end_frame / original_fps\n",
    "                abnormality_duration_seconds = abnormality_end_seconds - abnormality_start_seconds\n",
    "\n",
    "                print(f\"Abnormality {abnormality_group_id} start time: {int(abnormality_start_seconds // 60)} minutes {abnormality_start_seconds % 60:.2f} seconds\")\n",
    "                print(f\"Abnormality {abnormality_group_id} duration: {int(abnormality_duration_seconds // 60)} minutes {abnormality_duration_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    cap.release()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    total_time_seconds = frame_number / original_fps\n",
    "    print(f\"Total video time: {int(total_time_seconds // 60)} minutes {total_time_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    return abnormality_times\n",
    "\n",
    "# Example usage\n",
    "video_path = r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\C1007trimmed116-118.mp4\"\n",
    "output_dir = 'C:/Users/pkathi/Desktop/my work/computervision-video/cnntrail3'\n",
    "abnormality_times = motion_detection(video_path, output_dir, minimal_fps=5)\n",
    "\n",
    "if abnormality_times:\n",
    "    print(\"Abnormality detected at frame numbers:\", abnormality_times)\n",
    "else:\n",
    "    print(\"No abnormality detected in the video.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1767 images belonging to 3 classes.\n",
      "Found 440 images belonging to 3 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkathi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 1s/step - accuracy: 0.8760 - loss: 0.4585 - val_accuracy: 0.9477 - val_loss: 0.2094\n",
      "Epoch 2/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 1s/step - accuracy: 0.9446 - loss: 0.2252 - val_accuracy: 0.9477 - val_loss: 0.2071\n",
      "Epoch 3/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 929ms/step - accuracy: 0.9441 - loss: 0.2224 - val_accuracy: 0.9477 - val_loss: 0.2067\n",
      "Epoch 4/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 975ms/step - accuracy: 0.9487 - loss: 0.2136 - val_accuracy: 0.9477 - val_loss: 0.2066\n",
      "Epoch 5/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 1s/step - accuracy: 0.9334 - loss: 0.2531 - val_accuracy: 0.9477 - val_loss: 0.2087\n",
      "Epoch 6/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 993ms/step - accuracy: 0.9519 - loss: 0.2017 - val_accuracy: 0.9477 - val_loss: 0.2086\n",
      "Epoch 7/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - accuracy: 0.9403 - loss: 0.2314 - val_accuracy: 0.9477 - val_loss: 0.2062\n",
      "Epoch 8/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.9379 - loss: 0.2424 - val_accuracy: 0.9477 - val_loss: 0.2072\n",
      "Epoch 9/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 1s/step - accuracy: 0.9422 - loss: 0.2289 - val_accuracy: 0.9477 - val_loss: 0.2076\n",
      "Epoch 10/10\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 991ms/step - accuracy: 0.9416 - loss: 0.2294 - val_accuracy: 0.9477 - val_loss: 0.2149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "Classified as: smoke\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Define the model using EfficientNetB0\n",
    "def create_efficientnet_model(input_shape, num_classes):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')  # Number of classes\n",
    "    ])\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Prepare the dataset\n",
    "def prepare_dataset(data_dir, target_size=(224, 224), batch_size=32):\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, train_generator, validation_generator, epochs=10):\n",
    "    model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
    "\n",
    "# Use the model to classify abnormalities\n",
    "def classify_abnormality(model, frame):\n",
    "    frame_resized = cv2.resize(frame, (224, 224))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    classes = ['fire', 'smoke', 'sparkle']\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "# Example usage\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 3\n",
    "cnn_model = create_efficientnet_model(input_shape, num_classes)\n",
    "data_dir = r'C:\\Users\\pkathi\\Desktop\\object detection\\tradianal approach-cv\\Dataset' # Update this to the path of your dataset\n",
    "train_gen, val_gen = prepare_dataset(data_dir)\n",
    "train_model(cnn_model, train_gen, val_gen, epochs=10)\n",
    "\n",
    "# Save the trained model\n",
    "cnn_model.save('abnormality_classifier_efficientnet3.h5')\n",
    "\n",
    "# Example frame classification\n",
    "frame = cv2.imread(\"C:/Users/pkathi/Desktop/object detection/Data/test/images/frame_103434.jpg\")  # Replace with the path to a test frame\n",
    "classification = classify_abnormality(cnn_model, frame)\n",
    "print(f\"Classified as: {classification}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Abnormality 1 start time: 0 minutes 45.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 12.00 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "Abnormality 1 start time: 0 minutes 57.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Abnormality 1 start time: 0 minutes 57.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Abnormality 1 start time: 0 minutes 58.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "Total video time: 0 minutes 60.00 seconds\n",
      "Abnormality detected at frame numbers: [1350, 1722, 1734, 1746, 1758]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Define the model using EfficientNetB0\n",
    "def create_efficientnet_model(input_shape, num_classes):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')  # Number of classes\n",
    "    ])\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Prepare the dataset\n",
    "def prepare_dataset(data_dir, target_size=(224, 224), batch_size=32):\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, train_generator, validation_generator, epochs=10):\n",
    "    model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
    "\n",
    "# Use the model to classify abnormalities\n",
    "def classify_abnormality(model, frame):\n",
    "    frame_resized = cv2.resize(frame, (224, 224))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    classes = ['fire', 'smoke', 'fire+smoke', 'sparkle']\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "# Process frame with classification\n",
    "def process_frame(frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id, model):\n",
    "    frame_resized = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    frame_diff = cv2.absdiff(prev_frame_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = any(cv2.contourArea(contour) > min_contour_area for contour in contours)\n",
    "    if detected:\n",
    "        abnormality_class = classify_abnormality(model, frame)\n",
    "        abnormality_dir = os.path.join(output_dir, f\"abnormality_{abnormality_group_id}\")\n",
    "        os.makedirs(abnormality_dir, exist_ok=True)  # Ensure directory exists\n",
    "        output_frame_path = os.path.join(abnormality_dir, f\"{abnormality_class}_{frame_number}.jpg\")\n",
    "        cv2.imwrite(output_frame_path, frame_resized)\n",
    "        abnormality_time = frame_number / original_fps\n",
    "        timestamp_path = os.path.join(abnormality_dir, f\"{abnormality_class}_timestamps.txt\")\n",
    "        with open(timestamp_path, 'a') as f:\n",
    "            f.write(f\"{abnormality_class} - {abnormality_time:.2f} seconds\\n\")\n",
    "    \n",
    "    return detected, frame_gray\n",
    "\n",
    "# Motion detection function\n",
    "def motion_detection(video_path, output_dir, model_path, threshold_value=25, min_contour_area=500, resize_factor=0.5, minimal_fps=5, min_abnormality_gap_minutes=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip_interval = int(round(original_fps / minimal_fps))\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame\")\n",
    "        return\n",
    "\n",
    "    prev_frame = cv2.resize(prev_frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    abnormality_times = []\n",
    "\n",
    "    frame_number = 0\n",
    "    abnormality_detected = False\n",
    "    abnormality_start_frame = None\n",
    "    abnormality_group_id = 0\n",
    "    last_abnormality_time = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pool = ThreadPool(processes=4)  # Use a thread pool with 4 threads\n",
    "\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    while True:\n",
    "        for _ in range(frame_skip_interval):\n",
    "            ret = cap.grab()  # Use grab to skip frames efficiently\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_number += 1\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.retrieve()  # Retrieve the frame after skipping\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use thread pool to process frame\n",
    "        result = pool.apply_async(process_frame, (frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id, model))\n",
    "        detected, prev_frame_gray = result.get()\n",
    "\n",
    "        if detected:\n",
    "            current_abnormality_time = frame_number / original_fps\n",
    "\n",
    "            if not abnormality_detected:\n",
    "                if last_abnormality_time is None or (current_abnormality_time - last_abnormality_time) >= (min_abnormality_gap_minutes * 60):\n",
    "                    abnormality_group_id += 1\n",
    "\n",
    "                abnormality_start_frame = frame_number\n",
    "                abnormality_detected = True\n",
    "                abnormality_times.append(frame_number)\n",
    "                last_abnormality_time = current_abnormality_time\n",
    "\n",
    "        else:\n",
    "            if abnormality_detected:\n",
    "                abnormality_detected = False\n",
    "                abnormality_end_frame = frame_number\n",
    "                abnormality_start_seconds = abnormality_start_frame / original_fps\n",
    "                abnormality_end_seconds = abnormality_end_frame / original_fps\n",
    "                abnormality_duration_seconds = abnormality_end_seconds - abnormality_start_seconds\n",
    "\n",
    "                print(f\"Abnormality {abnormality_group_id} start time: {int(abnormality_start_seconds // 60)} minutes {abnormality_start_seconds % 60:.2f} seconds\")\n",
    "                print(f\"Abnormality {abnormality_group_id} duration: {int(abnormality_duration_seconds // 60)} minutes {abnormality_duration_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    cap.release()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    total_time_seconds = frame_number / original_fps\n",
    "    print(f\"Total video time: {int(total_time_seconds // 60)} minutes {total_time_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    return abnormality_times\n",
    "\n",
    "# Example usage\n",
    "video_path = r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\output48.mp4\"\n",
    "output_dir = 'C:/Users/pkathi/Desktop/my work/computervision-video/effiecient4'\n",
    "model_path = 'abnormality_classifier_efficientnet3.h5'  # Path to your trained model\n",
    "abnormality_times = motion_detection(video_path, output_dir, model_path, minimal_fps=5)\n",
    "\n",
    "if abnormality_times:\n",
    "    print(\"Abnormality detected at frame numbers:\", abnormality_times)\n",
    "else:\n",
    "    print(\"No abnormality detected in the video.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "Abnormality 1 start time: 0 minutes 57.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.80 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Abnormality 1 start time: 0 minutes 58.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Abnormality 1 start time: 1 minutes 3.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Abnormality 1 start time: 1 minutes 3.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Abnormality 1 start time: 1 minutes 17.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "Abnormality 1 start time: 1 minutes 17.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "Total video time: 1 minutes 60.00 seconds\n",
      "Abnormality detected at frame numbers: [1728, 1758, 1890, 1902, 2310, 2322]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Define the model using EfficientNetB0\n",
    "def create_efficientnet_model(input_shape, num_classes):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')  # Number of classes\n",
    "    ])\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Prepare the dataset\n",
    "def prepare_dataset(data_dir, target_size=(224, 224), batch_size=32):\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, train_generator, validation_generator, epochs=10):\n",
    "    model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
    "\n",
    "# Use the model to classify abnormalities\n",
    "def classify_abnormality(model, frame):\n",
    "    frame_resized = cv2.resize(frame, (224, 224))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    classes = ['fire', 'smoke', 'fire+smoke', 'sparkle']\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "# Process frame with classification\n",
    "def process_frame(frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id, model):\n",
    "    frame_resized = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    frame_diff = cv2.absdiff(prev_frame_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = any(cv2.contourArea(contour) > min_contour_area for contour in contours)\n",
    "    if detected:\n",
    "        abnormality_class = classify_abnormality(model, frame)\n",
    "        abnormality_dir = os.path.join(output_dir, f\"abnormality_{abnormality_group_id}\")\n",
    "        os.makedirs(abnormality_dir, exist_ok=True)  # Ensure directory exists\n",
    "        output_frame_path = os.path.join(abnormality_dir, f\"{abnormality_class}_{frame_number}.jpg\")\n",
    "        cv2.imwrite(output_frame_path, frame_resized)\n",
    "        abnormality_time = frame_number / original_fps\n",
    "        timestamp_path = os.path.join(abnormality_dir, f\"{abnormality_class}_timestamps.txt\")\n",
    "        with open(timestamp_path, 'a') as f:\n",
    "            f.write(f\"{abnormality_class} - {abnormality_time:.2f} seconds\\n\")\n",
    "    \n",
    "    return detected, frame_gray\n",
    "\n",
    "# Motion detection function\n",
    "def motion_detection(video_path, output_dir, model_path, threshold_value=25, min_contour_area=500, resize_factor=0.5, minimal_fps=5, min_abnormality_gap_minutes=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip_interval = int(round(original_fps / minimal_fps))\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame\")\n",
    "        return\n",
    "\n",
    "    prev_frame = cv2.resize(prev_frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    abnormality_times = []\n",
    "\n",
    "    frame_number = 0\n",
    "    abnormality_detected = False\n",
    "    abnormality_start_frame = None\n",
    "    abnormality_group_id = 0\n",
    "    last_abnormality_time = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pool = ThreadPool(processes=4)  # Use a thread pool with 4 threads\n",
    "\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    while True:\n",
    "        for _ in range(frame_skip_interval):\n",
    "            ret = cap.grab()  # Use grab to skip frames efficiently\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_number += 1\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.retrieve()  # Retrieve the frame after skipping\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use thread pool to process frame\n",
    "        result = pool.apply_async(process_frame, (frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id, model))\n",
    "        detected, prev_frame_gray = result.get()\n",
    "\n",
    "        if detected:\n",
    "            current_abnormality_time = frame_number / original_fps\n",
    "\n",
    "            if not abnormality_detected:\n",
    "                if last_abnormality_time is None or (current_abnormality_time - last_abnormality_time) >= (min_abnormality_gap_minutes * 60):\n",
    "                    abnormality_group_id += 1\n",
    "\n",
    "                abnormality_start_frame = frame_number\n",
    "                abnormality_detected = True\n",
    "                abnormality_times.append(frame_number)\n",
    "                last_abnormality_time = current_abnormality_time\n",
    "\n",
    "        else:\n",
    "            if abnormality_detected:\n",
    "                abnormality_detected = False\n",
    "                abnormality_end_frame = frame_number\n",
    "                abnormality_start_seconds = abnormality_start_frame / original_fps\n",
    "                abnormality_end_seconds = abnormality_end_frame / original_fps\n",
    "                abnormality_duration_seconds = abnormality_end_seconds - abnormality_start_seconds\n",
    "\n",
    "                print(f\"Abnormality {abnormality_group_id} start time: {int(abnormality_start_seconds // 60)} minutes {abnormality_start_seconds % 60:.2f} seconds\")\n",
    "                print(f\"Abnormality {abnormality_group_id} duration: {int(abnormality_duration_seconds // 60)} minutes {abnormality_duration_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    cap.release()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    total_time_seconds = frame_number / original_fps\n",
    "    print(f\"Total video time: {int(total_time_seconds // 60)} minutes {total_time_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    return abnormality_times\n",
    "\n",
    "# Example usage\n",
    "video_path =  r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\C1007trimmed116-118.mp4\"\n",
    "output_dir = 'C:/Users/pkathi/Desktop/my work/computervision-video/effiecient5'\n",
    "model_path = 'abnormality_classifier_efficientnet3.h5'  # Path to your trained model\n",
    "abnormality_times = motion_detection(video_path, output_dir, model_path, minimal_fps=5)\n",
    "\n",
    "if abnormality_times:\n",
    "    print(\"Abnormality detected at frame numbers:\", abnormality_times)\n",
    "else:\n",
    "    print(\"No abnormality detected in the video.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#above giving only smoke--make efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 142 images belonging to 3 classes.\n",
      "Found 33 images belonging to 3 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkathi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2s/step - accuracy: 0.4885 - loss: 1.0271 - val_accuracy: 0.4545 - val_loss: 0.8632\n",
      "Epoch 2/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 969ms/step - accuracy: 0.4758 - loss: 0.8511 - val_accuracy: 0.5455 - val_loss: 0.7621\n",
      "Epoch 3/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.5873 - loss: 0.7855 - val_accuracy: 0.5455 - val_loss: 0.7270\n",
      "Epoch 4/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.5471 - loss: 0.7536 - val_accuracy: 0.5455 - val_loss: 0.7104\n",
      "Epoch 5/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.4094 - loss: 0.7797 - val_accuracy: 0.4545 - val_loss: 0.7098\n",
      "Epoch 6/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 882ms/step - accuracy: 0.5028 - loss: 0.7583 - val_accuracy: 0.5455 - val_loss: 0.7014\n",
      "Epoch 7/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 866ms/step - accuracy: 0.5318 - loss: 0.7605 - val_accuracy: 0.5455 - val_loss: 0.7030\n",
      "Epoch 8/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 870ms/step - accuracy: 0.5116 - loss: 0.7773 - val_accuracy: 0.5455 - val_loss: 0.7121\n",
      "Epoch 9/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 889ms/step - accuracy: 0.5462 - loss: 0.7301 - val_accuracy: 0.5455 - val_loss: 0.7037\n",
      "Epoch 10/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 876ms/step - accuracy: 0.5193 - loss: 0.7549 - val_accuracy: 0.5455 - val_loss: 0.6998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model using EfficientNetB0\n",
    "def create_efficientnet_model(input_shape, num_classes):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')  # Number of classes\n",
    "    ])\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Prepare the dataset\n",
    "def prepare_dataset(data_dir, target_size=(224, 224), batch_size=32):\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, train_generator, validation_generator, epochs=10):\n",
    "    model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
    "    model.save('abnormality_classifier_efficientnet2.h5')\n",
    "\n",
    "# Example usage\n",
    "data_dir = r'C:\\Users\\pkathi\\Desktop\\object detection\\tradianal approach-cv\\Dataset' # Replace with your dataset directory\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 3\n",
    "\n",
    "model = create_efficientnet_model(input_shape, num_classes)\n",
    "train_generator, validation_generator = prepare_dataset(data_dir)\n",
    "train_model(model, train_generator, validation_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "Abnormality 1 start time: 0 minutes 57.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.80 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Abnormality 1 start time: 0 minutes 58.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Abnormality 1 start time: 1 minutes 3.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "Abnormality 1 start time: 1 minutes 3.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "Abnormality 1 start time: 1 minutes 17.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "Abnormality 1 start time: 1 minutes 17.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "Total video time: 1 minutes 60.00 seconds\n",
      "Abnormality detected at frame numbers [1728, 1758, 1890, 1902, 2310, 2322]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('abnormality_classifier_efficientnet2.h5')\n",
    "classes = ['fire', 'smoke', 'sparkle']\n",
    "\n",
    "def classify_abnormality(frame, model):\n",
    "    frame_resized = cv2.resize(frame, (224, 224))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "def process_frame(frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id, model):\n",
    "    frame_resized = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    frame_diff = cv2.absdiff(prev_frame_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = any(cv2.contourArea(contour) > min_contour_area for contour in contours)\n",
    "    if detected:\n",
    "        abnormality_class = classify_abnormality(frame, model)\n",
    "        abnormality_dir = os.path.join(output_dir, f\"abnormality_{abnormality_group_id}\")\n",
    "        os.makedirs(abnormality_dir, exist_ok=True)  # Ensure directory exists\n",
    "        output_frame_path = os.path.join(abnormality_dir, f\"{abnormality_class}_{frame_number}.jpg\")\n",
    "        cv2.imwrite(output_frame_path, frame_resized)\n",
    "        abnormality_time = frame_number / original_fps\n",
    "        timestamp_path = os.path.join(abnormality_dir, f\"{abnormality_class}_timestamps.txt\")\n",
    "        with open(timestamp_path, 'a') as f:\n",
    "            f.write(f\"{abnormality_class} - {abnormality_time:.2f} seconds\\n\")\n",
    "    \n",
    "    return detected, frame_gray\n",
    "\n",
    "def motion_detection(video_path, output_dir, model_path, threshold_value=25, min_contour_area=500, resize_factor=0.5, minimal_fps=5, min_abnormality_gap_minutes=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip_interval = int(round(original_fps / minimal_fps))\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame\")\n",
    "        return\n",
    "\n",
    "    prev_frame = cv2.resize(prev_frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    abnormality_times = []\n",
    "\n",
    "    frame_number = 0\n",
    "    abnormality_detected = False\n",
    "    abnormality_start_frame = None\n",
    "    abnormality_group_id = 0\n",
    "    last_abnormality_time = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pool = ThreadPool(processes=4)  # Use a thread pool with 4 threads\n",
    "\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    while True:\n",
    "        for _ in range(frame_skip_interval):\n",
    "            ret = cap.grab()  # Use grab to skip frames efficiently\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_number += 1\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.retrieve()  # Retrieve the frame after skipping\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use thread pool to process frame\n",
    "        result = pool.apply_async(process_frame, (frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id, model))\n",
    "        detected, prev_frame_gray = result.get()\n",
    "\n",
    "        if detected:\n",
    "            current_abnormality_time = frame_number / original_fps\n",
    "\n",
    "            if not abnormality_detected:\n",
    "                if last_abnormality_time is None or (current_abnormality_time - last_abnormality_time) >= (min_abnormality_gap_minutes * 60):\n",
    "                    abnormality_group_id += 1\n",
    "\n",
    "                abnormality_start_frame = frame_number\n",
    "                abnormality_detected = True\n",
    "                abnormality_times.append(frame_number)\n",
    "                last_abnormality_time = current_abnormality_time\n",
    "\n",
    "        else:\n",
    "            if abnormality_detected:\n",
    "                abnormality_detected = False\n",
    "                abnormality_end_frame = frame_number\n",
    "                abnormality_start_seconds = abnormality_start_frame / original_fps\n",
    "                abnormality_end_seconds = abnormality_end_frame / original_fps\n",
    "                abnormality_duration_seconds = abnormality_end_seconds - abnormality_start_seconds\n",
    "\n",
    "                print(f\"Abnormality {abnormality_group_id} start time: {int(abnormality_start_seconds // 60)} minutes {abnormality_start_seconds % 60:.2f} seconds\")\n",
    "                print(f\"Abnormality {abnormality_group_id} duration: {int(abnormality_duration_seconds // 60)} minutes {abnormality_duration_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    cap.release()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    total_time_seconds = frame_number / original_fps\n",
    "    print(f\"Total video time: {int(total_time_seconds // 60)} minutes {total_time_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    return abnormality_times\n",
    "\n",
    "# Example usage\n",
    "video_path =  r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\C1007trimmed116-118.mp4\"\n",
    "output_dir = 'C:/Users/pkathi/Desktop/my work/computervision-video/effiecient7'\n",
    "model_path = 'abnormality_classifier_efficientnet2.h5'  # Path to your trained model\n",
    "abnormality_times = motion_detection(video_path, output_dir, model_path, minimal_fps=5)\n",
    "\n",
    "if abnormality_times:\n",
    "    print(\"Abnormality detected at frame numbers\", abnormality_times)\n",
    "else:\n",
    "    print(\"No abnormality detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total video time: 0 minutes 60.00 seconds\n",
      "No abnormality detected.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('abnormality_classifier_efficientnet2.h5')\n",
    "classes = ['fire', 'smoke', 'sparkle']\n",
    "\n",
    "def classify_abnormality(frame, model):\n",
    "    frame_resized = cv2.resize(frame, (224, 224))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "def process_frame(frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id, model):\n",
    "    frame_resized = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    frame_diff = cv2.absdiff(prev_frame_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = any(cv2.contourArea(contour) > min_contour_area for contour in contours)\n",
    "    if detected:\n",
    "        abnormality_class = classify_abnormality(frame, model)\n",
    "        abnormality_dir = os.path.join(output_dir, f\"abnormality_{abnormality_group_id}\")\n",
    "        os.makedirs(abnormality_dir, exist_ok=True)  # Ensure directory exists\n",
    "        output_frame_path = os.path.join(abnormality_dir, f\"{abnormality_class}_{frame_number}.jpg\")\n",
    "        cv2.imwrite(output_frame_path, frame_resized)\n",
    "        abnormality_time = frame_number / original_fps\n",
    "        timestamp_path = os.path.join(abnormality_dir, f\"{abnormality_class}_timestamps.txt\")\n",
    "        with open(timestamp_path, 'a') as f:\n",
    "            f.write(f\"{abnormality_class} - {abnormality_time:.2f} seconds\\n\")\n",
    "    \n",
    "    return detected, frame_gray\n",
    "\n",
    "def motion_detection(video_path, output_dir, model_path, threshold_value=25, min_contour_area=500, resize_factor=0.5, minimal_fps=5, min_abnormality_gap_minutes=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip_interval = int(round(original_fps / minimal_fps))\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame\")\n",
    "        return\n",
    "\n",
    "    prev_frame = cv2.resize(prev_frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    abnormality_times = []\n",
    "\n",
    "    frame_number = 0\n",
    "    abnormality_detected = False\n",
    "    abnormality_start_frame = None\n",
    "    abnormality_group_id = 0\n",
    "    last_abnormality_time = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pool = ThreadPool(processes=4)  # Use a thread pool with 4 threads\n",
    "\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    while True:\n",
    "        for _ in range(frame_skip_interval):\n",
    "            ret = cap.grab()  # Use grab to skip frames efficiently\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_number += 1\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.retrieve()  # Retrieve the frame after skipping\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use thread pool to process frame\n",
    "        result = pool.apply_async(process_frame, (frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id, model))\n",
    "        detected, prev_frame_gray = result.get()\n",
    "\n",
    "        if detected:\n",
    "            current_abnormality_time = frame_number / original_fps\n",
    "\n",
    "            if not abnormality_detected:\n",
    "                if last_abnormality_time is None or (current_abnormality_time - last_abnormality_time) >= (min_abnormality_gap_minutes * 60):\n",
    "                    abnormality_group_id += 1\n",
    "\n",
    "                abnormality_start_frame = frame_number\n",
    "                abnormality_detected = True\n",
    "                abnormality_times.append(frame_number)\n",
    "                last_abnormality_time = current_abnormality_time\n",
    "\n",
    "        else:\n",
    "            if abnormality_detected:\n",
    "                abnormality_detected = False\n",
    "                abnormality_end_frame = frame_number\n",
    "                abnormality_start_seconds = abnormality_start_frame / original_fps\n",
    "                abnormality_end_seconds = abnormality_end_frame / original_fps\n",
    "                abnormality_duration_seconds = abnormality_end_seconds - abnormality_start_seconds\n",
    "\n",
    "                print(f\"Abnormality {abnormality_group_id} start time: {int(abnormality_start_seconds // 60)} minutes {abnormality_start_seconds % 60:.2f} seconds\")\n",
    "                print(f\"Abnormality {abnormality_group_id} duration: {int(abnormality_duration_seconds // 60)} minutes {abnormality_duration_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    cap.release()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    total_time_seconds = frame_number / original_fps\n",
    "    print(f\"Total video time: {int(total_time_seconds // 60)} minutes {total_time_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    return abnormality_times\n",
    "\n",
    "# Example usage\n",
    "video_path =  r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\noabnormality.mp4\"\n",
    "output_dir = 'C:/Users/pkathi/Desktop/my work/computervision-video/effiecient7'\n",
    "model_path = 'abnormality_classifier_efficientnet2.h5'  # Path to your trained model\n",
    "abnormality_times = motion_detection(video_path, output_dir, model_path, minimal_fps=5)\n",
    "\n",
    "if abnormality_times:\n",
    "    print(\"Abnormality detected at frame numbers\", abnormality_times)\n",
    "else:\n",
    "    print(\"No abnormality detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "Abnormality 1 start time: 0 minutes 51.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 1.00 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "Abnormality 1 start time: 0 minutes 52.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.60 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "Abnormality 1 start time: 0 minutes 55.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Abnormality 1 start time: 0 minutes 56.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 2.00 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step\n",
      "Abnormality 1 start time: 0 minutes 58.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 3.40 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "Abnormality 1 start time: 1 minutes 2.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.40 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "Abnormality 1 start time: 1 minutes 2.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "Abnormality 1 start time: 1 minutes 7.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "Abnormality 1 start time: 1 minutes 7.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.40 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "Abnormality 1 start time: 1 minutes 8.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.80 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "Abnormality 1 start time: 1 minutes 10.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "Abnormality 1 start time: 1 minutes 12.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 2.00 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "Abnormality 1 start time: 1 minutes 15.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.80 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "Abnormality 1 start time: 1 minutes 16.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 1.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "Abnormality 1 start time: 1 minutes 18.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Abnormality 1 start time: 1 minutes 19.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "Total video time: 1 minutes 60.00 seconds\n",
      "Abnormality detected at frame numbers [1530, 1584, 1656, 1686, 1752, 1860, 1884, 2016, 2028, 2064, 2124, 2172, 2250, 2304, 2352, 2376]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('abnormality_classifier_efficientnet2.h5')\n",
    "classes = ['fire', 'smoke', 'sparkle']\n",
    "\n",
    "def classify_abnormality(frame, model):\n",
    "    frame_resized = cv2.resize(frame, (224, 224))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "def process_frame(frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id, model):\n",
    "    frame_resized = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    frame_diff = cv2.absdiff(prev_frame_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = any(cv2.contourArea(contour) > min_contour_area for contour in contours)\n",
    "    if detected:\n",
    "        abnormality_class = classify_abnormality(frame, model)\n",
    "        abnormality_dir = os.path.join(output_dir, f\"abnormality_{abnormality_group_id}\")\n",
    "        os.makedirs(abnormality_dir, exist_ok=True)  # Ensure directory exists\n",
    "        output_frame_path = os.path.join(abnormality_dir, f\"{abnormality_class}_{frame_number}.jpg\")\n",
    "        cv2.imwrite(output_frame_path, frame_resized)\n",
    "        abnormality_time = frame_number / original_fps\n",
    "        timestamp_path = os.path.join(abnormality_dir, f\"{abnormality_class}_timestamps.txt\")\n",
    "        with open(timestamp_path, 'a') as f:\n",
    "            f.write(f\"{abnormality_class} - {abnormality_time:.2f} seconds\\n\")\n",
    "    \n",
    "    return detected, frame_gray\n",
    "\n",
    "def motion_detection(video_path, output_dir, model_path, threshold_value=25, min_contour_area=500, resize_factor=0.5, minimal_fps=5, min_abnormality_gap_minutes=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip_interval = int(round(original_fps / minimal_fps))\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame\")\n",
    "        return\n",
    "\n",
    "    prev_frame = cv2.resize(prev_frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    abnormality_times = []\n",
    "\n",
    "    frame_number = 0\n",
    "    abnormality_detected = False\n",
    "    abnormality_start_frame = None\n",
    "    abnormality_group_id = 0\n",
    "    last_abnormality_time = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pool = ThreadPool(processes=4)  # Use a thread pool with 4 threads\n",
    "\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    while True:\n",
    "        for _ in range(frame_skip_interval):\n",
    "            ret = cap.grab()  # Use grab to skip frames efficiently\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_number += 1\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.retrieve()  # Retrieve the frame after skipping\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use thread pool to process frame\n",
    "        result = pool.apply_async(process_frame, (frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id, model))\n",
    "        detected, prev_frame_gray = result.get()\n",
    "\n",
    "        if detected:\n",
    "            current_abnormality_time = frame_number / original_fps\n",
    "\n",
    "            if not abnormality_detected:\n",
    "                if last_abnormality_time is None or (current_abnormality_time - last_abnormality_time) >= (min_abnormality_gap_minutes * 60):\n",
    "                    abnormality_group_id += 1\n",
    "\n",
    "                abnormality_start_frame = frame_number\n",
    "                abnormality_detected = True\n",
    "                abnormality_times.append(frame_number)\n",
    "                last_abnormality_time = current_abnormality_time\n",
    "\n",
    "        else:\n",
    "            if abnormality_detected:\n",
    "                abnormality_detected = False\n",
    "                abnormality_end_frame = frame_number\n",
    "                abnormality_start_seconds = abnormality_start_frame / original_fps\n",
    "                abnormality_end_seconds = abnormality_end_frame / original_fps\n",
    "                abnormality_duration_seconds = abnormality_end_seconds - abnormality_start_seconds\n",
    "\n",
    "                print(f\"Abnormality {abnormality_group_id} start time: {int(abnormality_start_seconds // 60)} minutes {abnormality_start_seconds % 60:.2f} seconds\")\n",
    "                print(f\"Abnormality {abnormality_group_id} duration: {int(abnormality_duration_seconds // 60)} minutes {abnormality_duration_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    cap.release()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    total_time_seconds = frame_number / original_fps\n",
    "    print(f\"Total video time: {int(total_time_seconds // 60)} minutes {total_time_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    return abnormality_times\n",
    "\n",
    "# Example usage\n",
    "video_path =  r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\2-DMG_RSA_HTE101_D_HS_FIRE_31-33min\\output31.mp4\"\n",
    "output_dir = 'C:/Users/pkathi/Desktop/my work/computervision-video/effiecient8'\n",
    "model_path = 'abnormality_classifier_efficientnet2.h5'  # Path to your trained model\n",
    "abnormality_times = motion_detection(video_path, output_dir, model_path, minimal_fps=5)\n",
    "\n",
    "if abnormality_times:\n",
    "    print(\"Abnormality detected at frame numbers\", abnormality_times)\n",
    "else:\n",
    "    print(\"No abnormality detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 219 images belonging to 2 classes.\n",
      "Found 54 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2s/step - accuracy: 0.4777 - loss: 0.6896 - val_accuracy: 0.6111 - val_loss: 0.6694\n",
      "Epoch 2/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.5763 - loss: 0.6835 - val_accuracy: 0.6111 - val_loss: 0.6702\n",
      "Epoch 3/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.6233 - loss: 0.6692 - val_accuracy: 0.6111 - val_loss: 0.6689\n",
      "Epoch 4/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.6048 - loss: 0.6766 - val_accuracy: 0.6111 - val_loss: 0.6692\n",
      "Epoch 5/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.5943 - loss: 0.6862 - val_accuracy: 0.6111 - val_loss: 0.6693\n",
      "Epoch 6/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.6110 - loss: 0.6703 - val_accuracy: 0.6111 - val_loss: 0.6688\n",
      "Epoch 7/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.6218 - loss: 0.6632 - val_accuracy: 0.6111 - val_loss: 0.6687\n",
      "Epoch 8/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.6007 - loss: 0.6717 - val_accuracy: 0.6111 - val_loss: 0.6703\n",
      "Epoch 9/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.5817 - loss: 0.6798 - val_accuracy: 0.6111 - val_loss: 0.6689\n",
      "Epoch 10/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.5904 - loss: 0.6770 - val_accuracy: 0.6111 - val_loss: 0.6684\n",
      "Epoch 11/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.6091 - loss: 0.6694 - val_accuracy: 0.6111 - val_loss: 0.6685\n",
      "Epoch 12/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.6354 - loss: 0.6547 - val_accuracy: 0.6111 - val_loss: 0.6689\n",
      "Epoch 13/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.6294 - loss: 0.6630 - val_accuracy: 0.6111 - val_loss: 0.6706\n",
      "Epoch 14/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.6024 - loss: 0.6751 - val_accuracy: 0.6111 - val_loss: 0.6694\n",
      "Epoch 15/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.6124 - loss: 0.6710 - val_accuracy: 0.6111 - val_loss: 0.6686\n",
      "Epoch 16/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.5792 - loss: 0.6838 - val_accuracy: 0.6111 - val_loss: 0.6686\n",
      "Epoch 17/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.5882 - loss: 0.6776 - val_accuracy: 0.6111 - val_loss: 0.6685\n",
      "Epoch 18/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.6063 - loss: 0.6712 - val_accuracy: 0.6111 - val_loss: 0.6686\n",
      "Epoch 19/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.6269 - loss: 0.6651 - val_accuracy: 0.6111 - val_loss: 0.6685\n",
      "Epoch 20/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.5905 - loss: 0.6800 - val_accuracy: 0.6111 - val_loss: 0.6699\n",
      "Epoch 21/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.6195 - loss: 0.6673 - val_accuracy: 0.6111 - val_loss: 0.6686\n",
      "Epoch 22/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.5884 - loss: 0.6825 - val_accuracy: 0.6111 - val_loss: 0.6693\n",
      "Epoch 23/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.5844 - loss: 0.6766 - val_accuracy: 0.6111 - val_loss: 0.6687\n",
      "Epoch 24/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.6088 - loss: 0.6685 - val_accuracy: 0.6111 - val_loss: 0.6688\n",
      "Epoch 25/25\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.5746 - loss: 0.6895 - val_accuracy: 0.6111 - val_loss: 0.6705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model using EfficientNetB0\n",
    "def create_efficientnet_model(input_shape, num_classes):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')  # Number of classes\n",
    "    ])\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Prepare the dataset\n",
    "def prepare_dataset(data_dir, target_size=(224, 224), batch_size=32):\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, train_generator, validation_generator, epochs=25):\n",
    "    model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
    "    model.save('abnormality_classifier_efficientnet3.h5')\n",
    "\n",
    "# Example usage\n",
    "data_dir = r\"C:\\Users\\pkathi\\Desktop\\object detection\\tradianal approach-cv\\Dataset\" # Replace with your dataset directory\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 2\n",
    "\n",
    "model = create_efficientnet_model(input_shape, num_classes)\n",
    "train_generator, validation_generator = prepare_dataset(data_dir)\n",
    "train_model(model, train_generator, validation_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Abnormality 1 start time: 0 minutes 51.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 1.00 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "Abnormality 1 start time: 0 minutes 52.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.60 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Abnormality 1 start time: 0 minutes 55.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "Abnormality 1 start time: 0 minutes 56.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 2.00 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "Abnormality 1 start time: 0 minutes 58.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 3.40 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "Abnormality 1 start time: 1 minutes 2.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.40 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "Abnormality 1 start time: 1 minutes 2.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Abnormality 1 start time: 1 minutes 7.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "Abnormality 1 start time: 1 minutes 7.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.40 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "Abnormality 1 start time: 1 minutes 8.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.80 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
      "Abnormality 1 start time: 1 minutes 10.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "Abnormality 1 start time: 1 minutes 12.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 2.00 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Abnormality 1 start time: 1 minutes 15.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.80 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "Abnormality 1 start time: 1 minutes 16.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 1.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "Abnormality 1 start time: 1 minutes 18.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "Abnormality 1 start time: 1 minutes 19.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "Total video time: 1 minutes 60.00 seconds\n",
      "Abnormality detected at frame numbers [1530, 1584, 1656, 1686, 1752, 1860, 1884, 2016, 2028, 2064, 2124, 2172, 2250, 2304, 2352, 2376]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('abnormality_classifier_efficientnet2.h5')\n",
    "classes = ['fire', 'smoke', 'sparkle']\n",
    "\n",
    "def classify_abnormality(frame, model):\n",
    "    frame_resized = cv2.resize(frame, (224, 224))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "def process_frame(frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id, model):\n",
    "    frame_resized = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    frame_diff = cv2.absdiff(prev_frame_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = any(cv2.contourArea(contour) > min_contour_area for contour in contours)\n",
    "    if detected:\n",
    "        abnormality_class = classify_abnormality(frame, model)\n",
    "        abnormality_dir = os.path.join(output_dir, f\"abnormality_{abnormality_group_id}\")\n",
    "        os.makedirs(abnormality_dir, exist_ok=True)  # Ensure directory exists\n",
    "        output_frame_path = os.path.join(abnormality_dir, f\"{abnormality_class}_{frame_number}.jpg\")\n",
    "        cv2.imwrite(output_frame_path, frame_resized)\n",
    "        abnormality_time = frame_number / original_fps\n",
    "        timestamp_path = os.path.join(abnormality_dir, f\"{abnormality_class}_timestamps.txt\")\n",
    "        with open(timestamp_path, 'a') as f:\n",
    "            f.write(f\"{abnormality_class} - {abnormality_time:.2f} seconds\\n\")\n",
    "    \n",
    "    return detected, frame_gray\n",
    "\n",
    "def motion_detection(video_path, output_dir, model_path, threshold_value=25, min_contour_area=500, resize_factor=0.5, minimal_fps=5, min_abnormality_gap_minutes=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip_interval = int(round(original_fps / minimal_fps))\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame\")\n",
    "        return\n",
    "\n",
    "    prev_frame = cv2.resize(prev_frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    abnormality_times = []\n",
    "\n",
    "    frame_number = 0\n",
    "    abnormality_detected = False\n",
    "    abnormality_start_frame = None\n",
    "    abnormality_group_id = 0\n",
    "    last_abnormality_time = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pool = ThreadPool(processes=4)  # Use a thread pool with 4 threads\n",
    "\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    while True:\n",
    "        for _ in range(frame_skip_interval):\n",
    "            ret = cap.grab()  # Use grab to skip frames efficiently\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_number += 1\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.retrieve()  # Retrieve the frame after skipping\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use thread pool to process frame\n",
    "        result = pool.apply_async(process_frame, (frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id, model))\n",
    "        detected, prev_frame_gray = result.get()\n",
    "\n",
    "        if detected:\n",
    "            current_abnormality_time = frame_number / original_fps\n",
    "\n",
    "            if not abnormality_detected:\n",
    "                if last_abnormality_time is None or (current_abnormality_time - last_abnormality_time) >= (min_abnormality_gap_minutes * 60):\n",
    "                    abnormality_group_id += 1\n",
    "\n",
    "                abnormality_start_frame = frame_number\n",
    "                abnormality_detected = True\n",
    "                abnormality_times.append(frame_number)\n",
    "                last_abnormality_time = current_abnormality_time\n",
    "\n",
    "        else:\n",
    "            if abnormality_detected:\n",
    "                abnormality_detected = False\n",
    "                abnormality_end_frame = frame_number\n",
    "                abnormality_start_seconds = abnormality_start_frame / original_fps\n",
    "                abnormality_end_seconds = abnormality_end_frame / original_fps\n",
    "                abnormality_duration_seconds = abnormality_end_seconds - abnormality_start_seconds\n",
    "\n",
    "                print(f\"Abnormality {abnormality_group_id} start time: {int(abnormality_start_seconds // 60)} minutes {abnormality_start_seconds % 60:.2f} seconds\")\n",
    "                print(f\"Abnormality {abnormality_group_id} duration: {int(abnormality_duration_seconds // 60)} minutes {abnormality_duration_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    cap.release()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    total_time_seconds = frame_number / original_fps\n",
    "    print(f\"Total video time: {int(total_time_seconds // 60)} minutes {total_time_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    return abnormality_times\n",
    "\n",
    "# Example usage\n",
    "video_path =  r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\2-DMG_RSA_HTE101_D_HS_FIRE_31-33min\\output31.mp4\"\n",
    "output_dir = 'C:/Users/pkathi/Desktop/my work/computervision-video/effiecient9'\n",
    "model_path = 'abnormality_classifier_efficientnet3.h5'  # Path to your trained model\n",
    "abnormality_times = motion_detection(video_path, output_dir, model_path, minimal_fps=5)\n",
    "\n",
    "if abnormality_times:\n",
    "    print(\"Abnormality detected at frame numbers\", abnormality_times)\n",
    "else:\n",
    "    print(\"No abnormality detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''With DATA AUGMENTATION'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 228 images belonging to 2 classes.\n",
      "Found 56 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkathi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 195ms/step - accuracy: 0.5775 - loss: 0.6844 - val_accuracy: 0.7143 - val_loss: 0.6068\n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.6873 - loss: 0.6095 - val_accuracy: 0.7143 - val_loss: 0.5956\n",
      "Epoch 3/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.6705 - loss: 0.6267 - val_accuracy: 0.7143 - val_loss: 0.6340\n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.7430 - loss: 0.5411 - val_accuracy: 0.7143 - val_loss: 0.6134\n",
      "Epoch 5/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.7312 - loss: 0.5465 - val_accuracy: 0.7143 - val_loss: 0.5936\n",
      "Epoch 6/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.7221 - loss: 0.5223 - val_accuracy: 0.7143 - val_loss: 0.5931\n",
      "Epoch 7/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.7026 - loss: 0.4842 - val_accuracy: 0.7143 - val_loss: 0.8677\n",
      "Epoch 8/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.6884 - loss: 0.6162 - val_accuracy: 0.7143 - val_loss: 0.5364\n",
      "Epoch 9/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.7771 - loss: 0.6012 - val_accuracy: 0.7143 - val_loss: 0.9011\n",
      "Epoch 10/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7949 - loss: 0.4116 - val_accuracy: 0.2857 - val_loss: 0.7178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: {'Fire': 0, 'Smoke': 1}\n",
      "Number of training samples: 228\n",
      "Number of validation samples: 56\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Define a simple CNN model\n",
    "def create_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(2, activation='softmax')  # 2 classes: fire, smoke\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Prepare the dataset with data augmentation\n",
    "def prepare_dataset(data_dir, target_size=(64, 64), batch_size=32):\n",
    "    # Data augmentation parameters\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255.0,\n",
    "        validation_split=0.2,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    validation_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255.0,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, train_generator, validation_generator, epochs=10):\n",
    "    model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
    "\n",
    "# Use the model to classify abnormalities\n",
    "def classify_abnormality(model, frame):\n",
    "    frame_resized = cv2.resize(frame, (64, 64))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    classes = ['Fire', 'Smoke']\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "# Example usage\n",
    "input_shape = (64, 64, 3)\n",
    "cnn_model = create_cnn_model(input_shape)\n",
    "data_dir = r'C:\\Users\\pkathi\\Desktop\\object detection\\tradianal approach-cv\\Dataset2'\n",
    "train_gen, val_gen = prepare_dataset(data_dir)\n",
    "train_model(cnn_model, train_gen, val_gen)\n",
    "\n",
    "# Save the trained model\n",
    "cnn_model.save('abnormality_classifier_augment2.h5')\n",
    "\n",
    "# Verify the dataset preparation\n",
    "print(f\"Classes found: {train_gen.class_indices}\")\n",
    "print(f\"Number of training samples: {train_gen.samples}\")\n",
    "print(f\"Number of validation samples: {val_gen.samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
      "Abnormality classified as: Fire at frame 1350\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Abnormality classified as: Fire at frame 1356\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Abnormality classified as: Fire at frame 1362\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Abnormality classified as: Fire at frame 1368\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Abnormality classified as: Fire at frame 1374\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Abnormality classified as: Fire at frame 1380\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Abnormality classified as: Fire at frame 1386\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Abnormality classified as: Fire at frame 1392\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Abnormality classified as: Fire at frame 1398\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Abnormality classified as: Fire at frame 1404\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Abnormality classified as: Fire at frame 1410\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Abnormality classified as: Fire at frame 1416\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Abnormality classified as: Fire at frame 1422\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Abnormality classified as: Fire at frame 1428\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Abnormality classified as: Fire at frame 1434\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Abnormality classified as: Fire at frame 1440\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Abnormality classified as: Fire at frame 1446\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Abnormality classified as: Fire at frame 1452\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Abnormality classified as: Fire at frame 1458\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Abnormality classified as: Fire at frame 1464\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Abnormality classified as: Fire at frame 1470\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Abnormality classified as: Fire at frame 1476\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Abnormality classified as: Fire at frame 1482\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Abnormality classified as: Fire at frame 1488\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Abnormality classified as: Fire at frame 1494\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Abnormality classified as: Fire at frame 1500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Abnormality classified as: Fire at frame 1506\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Abnormality classified as: Fire at frame 1512\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Abnormality classified as: Fire at frame 1518\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Abnormality classified as: Fire at frame 1524\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Abnormality classified as: Fire at frame 1530\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Abnormality classified as: Fire at frame 1536\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Abnormality classified as: Fire at frame 1542\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Abnormality classified as: Fire at frame 1548\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Abnormality classified as: Fire at frame 1554\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Abnormality classified as: Fire at frame 1560\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Abnormality classified as: Fire at frame 1566\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Abnormality classified as: Fire at frame 1572\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Abnormality classified as: Fire at frame 1578\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Abnormality classified as: Fire at frame 1584\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Abnormality classified as: Fire at frame 1590\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Abnormality classified as: Fire at frame 1596\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Abnormality classified as: Fire at frame 1602\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Abnormality classified as: Fire at frame 1608\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Abnormality classified as: Fire at frame 1614\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Abnormality classified as: Fire at frame 1620\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Abnormality classified as: Fire at frame 1626\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Abnormality classified as: Fire at frame 1632\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Abnormality classified as: Fire at frame 1638\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Abnormality classified as: Fire at frame 1644\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Abnormality classified as: Fire at frame 1650\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Abnormality classified as: Fire at frame 1656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Abnormality classified as: Fire at frame 1662\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Abnormality classified as: Fire at frame 1668\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Abnormality classified as: Fire at frame 1674\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Abnormality classified as: Fire at frame 1680\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Abnormality classified as: Fire at frame 1686\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Abnormality classified as: Fire at frame 1692\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Abnormality classified as: Fire at frame 1698\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Abnormality classified as: Fire at frame 1704\n",
      "Abnormality 1 start time: 0 minutes 45.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 12.00 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Abnormality classified as: Fire at frame 1722\n",
      "Abnormality 1 start time: 0 minutes 57.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Abnormality classified as: Fire at frame 1734\n",
      "Abnormality 1 start time: 0 minutes 57.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Abnormality classified as: Fire at frame 1746\n",
      "Abnormality 1 start time: 0 minutes 58.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Abnormality classified as: Fire at frame 1758\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Abnormality classified as: Fire at frame 1764\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Abnormality classified as: Fire at frame 1770\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Abnormality classified as: Fire at frame 1776\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Abnormality classified as: Fire at frame 1782\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Abnormality classified as: Fire at frame 1788\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Abnormality classified as: Fire at frame 1794\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Abnormality classified as: Fire at frame 1800\n",
      "Total video time: 0 minutes 60.00 seconds\n",
      "Abnormality detected at frame numbers: [1350, 1722, 1734, 1746, 1758]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Load the trained model\n",
    "cnn_model = load_model('abnormality_classifier_augment2.h5')\n",
    "\n",
    "# Define function to classify abnormality\n",
    "def classify_abnormality(model, frame):\n",
    "    frame_resized = cv2.resize(frame, (64, 64))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    classes = ['Fire', 'Smoke']\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "# Update the process_frame function\n",
    "def process_frame(frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id):\n",
    "    frame_resized = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    frame_diff = cv2.absdiff(prev_frame_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = any(cv2.contourArea(contour) > min_contour_area for contour in contours)\n",
    "    if detected:\n",
    "        output_frame_path = os.path.join(output_dir, f\"abnormality_{abnormality_group_id}\", f\"frame_{frame_number}.jpg\")\n",
    "        cv2.imwrite(output_frame_path, frame_resized)\n",
    "        \n",
    "        # Classify the abnormality\n",
    "        classification = classify_abnormality(cnn_model, frame_resized)\n",
    "        print(f\"Abnormality classified as: {classification} at frame {frame_number}\")\n",
    "\n",
    "    return detected, frame_gray\n",
    "\n",
    "# Motion detection function remains the same\n",
    "def motion_detection(video_path, output_dir, threshold_value=25, min_contour_area=500, resize_factor=0.5, minimal_fps=5, min_abnormality_gap_minutes=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip_interval = int(round(original_fps / minimal_fps))\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame\")\n",
    "        return\n",
    "\n",
    "    prev_frame = cv2.resize(prev_frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    abnormality_times = []\n",
    "\n",
    "    frame_number = 0\n",
    "    abnormality_detected = False\n",
    "    abnormality_start_frame = None\n",
    "    abnormality_group_id = 0\n",
    "    last_abnormality_time = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pool = ThreadPool(processes=4)  # Use a thread pool with 4 threads\n",
    "\n",
    "    while True:\n",
    "        for _ in range(frame_skip_interval):\n",
    "            ret = cap.grab()  # Use grab to skip frames efficiently\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_number += 1\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.retrieve()  # Retrieve the frame after skipping\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use thread pool to process frame\n",
    "        result = pool.apply_async(process_frame, (frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, abnormality_group_id))\n",
    "        detected, prev_frame_gray = result.get()\n",
    "\n",
    "        if detected:\n",
    "            current_abnormality_time = frame_number / original_fps\n",
    "\n",
    "            if not abnormality_detected:\n",
    "                if last_abnormality_time is None or (current_abnormality_time - last_abnormality_time) >= (min_abnormality_gap_minutes * 60):\n",
    "                    abnormality_group_id += 1\n",
    "                    os.makedirs(os.path.join(output_dir, f\"abnormality_{abnormality_group_id}\"))\n",
    "\n",
    "                abnormality_start_frame = frame_number\n",
    "                abnormality_detected = True\n",
    "                abnormality_times.append(frame_number)\n",
    "                last_abnormality_time = current_abnormality_time\n",
    "\n",
    "        else:\n",
    "            if abnormality_detected:\n",
    "                abnormality_detected = False\n",
    "                abnormality_end_frame = frame_number\n",
    "                abnormality_start_seconds = abnormality_start_frame / original_fps\n",
    "                abnormality_end_seconds = abnormality_end_frame / original_fps\n",
    "                abnormality_duration_seconds = abnormality_end_seconds - abnormality_start_seconds\n",
    "\n",
    "                print(f\"Abnormality {abnormality_group_id} start time: {int(abnormality_start_seconds // 60)} minutes {abnormality_start_seconds % 60:.2f} seconds\")\n",
    "                print(f\"Abnormality {abnormality_group_id} duration: {int(abnormality_duration_seconds // 60)} minutes {abnormality_duration_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    cap.release()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    total_time_seconds = frame_number / original_fps\n",
    "    print(f\"Total video time: {int(total_time_seconds // 60)} minutes {total_time_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    return abnormality_times\n",
    "\n",
    "# Example usage\n",
    "video_path = r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\output48.mp4\"\n",
    "output_dir = 'C:/Users/pkathi/Desktop/my work/computervision-video/augmrnt2_0'\n",
    "abnormality_times = motion_detection(video_path, output_dir, minimal_fps=5)\n",
    "\n",
    "if abnormality_times:\n",
    "    print(\"Abnormality detected at frame numbers:\", abnormality_times)\n",
    "else:\n",
    "    print(\"No abnormality detected in the video.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "Fire detected at 0m_45.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Fire detected at 0m_45.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Fire detected at 0m_45.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Fire detected at 0m_45.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Fire detected at 0m_45.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Fire detected at 0m_46.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Fire detected at 0m_46.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Fire detected at 0m_46.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Fire detected at 0m_46.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Fire detected at 0m_46.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Fire detected at 0m_47.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Fire detected at 0m_47.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Fire detected at 0m_47.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Fire detected at 0m_47.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Fire detected at 0m_47.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Fire detected at 0m_48.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Fire detected at 0m_48.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Fire detected at 0m_48.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Fire detected at 0m_48.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Fire detected at 0m_48.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Fire detected at 0m_49.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Fire detected at 0m_49.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Fire detected at 0m_49.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Fire detected at 0m_49.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Fire detected at 0m_49.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Fire detected at 0m_50.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Fire detected at 0m_50.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Fire detected at 0m_50.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Fire detected at 0m_50.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Fire detected at 0m_50.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Fire detected at 0m_51.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Fire detected at 0m_51.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Fire detected at 0m_51.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Fire detected at 0m_51.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
      "Fire detected at 0m_51.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Fire detected at 0m_52.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Fire detected at 0m_52.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Fire detected at 0m_52.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Fire detected at 0m_52.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Fire detected at 0m_52.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Fire detected at 0m_53.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Fire detected at 0m_53.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Fire detected at 0m_53.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Fire detected at 0m_53.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Fire detected at 0m_53.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Fire detected at 0m_54.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Fire detected at 0m_54.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Fire detected at 0m_54.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Fire detected at 0m_54.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Fire detected at 0m_54.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Fire detected at 0m_55.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Fire detected at 0m_55.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Fire detected at 0m_55.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Fire detected at 0m_55.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Fire detected at 0m_55.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Fire detected at 0m_56.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Fire detected at 0m_56.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Fire detected at 0m_56.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Fire detected at 0m_56.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Fire detected at 0m_56.80s\n",
      "Abnormality 1 start time: 0 minutes 45.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 12.00 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Fire detected at 0m_57.40s\n",
      "Abnormality 1 start time: 0 minutes 57.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Fire detected at 0m_57.80s\n",
      "Abnormality 1 start time: 0 minutes 57.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Fire detected at 0m_58.20s\n",
      "Abnormality 1 start time: 0 minutes 58.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Fire detected at 0m_58.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Fire detected at 0m_58.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Fire detected at 0m_59.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Fire detected at 0m_59.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Fire detected at 0m_59.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Fire detected at 0m_59.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Fire detected at 0m_59.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Fire detected at 0m_60.00s\n",
      "Total video time: 0 minutes 60.00 seconds\n",
      "Abnormality detected at frame numbers: [1350, 1722, 1734, 1746, 1758]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Load the trained model\n",
    "cnn_model = load_model('abnormality_classifier_augment2.h5')\n",
    "\n",
    "# Define function to classify abnormality\n",
    "def classify_abnormality(model, frame):\n",
    "    frame_resized = cv2.resize(frame, (64, 64))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    classes = ['Fire', 'Smoke']\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "# Update the process_frame function\n",
    "def process_frame(frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir):\n",
    "    frame_resized = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    frame_diff = cv2.absdiff(prev_frame_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = any(cv2.contourArea(contour) > min_contour_area for contour in contours)\n",
    "    if detected:\n",
    "        # Classify the abnormality\n",
    "        classification = classify_abnormality(cnn_model, frame_resized)\n",
    "        \n",
    "        abnormality_time = frame_number / original_fps\n",
    "        timestamp = f\"{int(abnormality_time // 60)}m_{abnormality_time % 60:.2f}s\"\n",
    "        output_frame_path = os.path.join(output_dir, classification, f\"{classification}_{timestamp}_frame_{frame_number}.jpg\")\n",
    "\n",
    "        os.makedirs(os.path.dirname(output_frame_path), exist_ok=True)\n",
    "        cv2.imwrite(output_frame_path, frame_resized)\n",
    "\n",
    "        print(f\"{classification} detected at {timestamp}\")\n",
    "    \n",
    "    return detected, frame_gray\n",
    "\n",
    "# Motion detection function\n",
    "def motion_detection(video_path, output_dir, threshold_value=25, min_contour_area=500, resize_factor=0.5, minimal_fps=5, min_abnormality_gap_minutes=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip_interval = int(round(original_fps / minimal_fps))\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame\")\n",
    "        return\n",
    "\n",
    "    prev_frame = cv2.resize(prev_frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    abnormality_times = []\n",
    "\n",
    "    frame_number = 0\n",
    "    abnormality_detected = False\n",
    "    abnormality_start_frame = None\n",
    "    abnormality_group_id = 0\n",
    "    last_abnormality_time = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pool = ThreadPool(processes=4)  # Use a thread pool with 4 threads\n",
    "\n",
    "    while True:\n",
    "        for _ in range(frame_skip_interval):\n",
    "            ret = cap.grab()  # Use grab to skip frames efficiently\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_number += 1\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.retrieve()  # Retrieve the frame after skipping\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use thread pool to process frame\n",
    "        result = pool.apply_async(process_frame, (frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir))\n",
    "        detected, prev_frame_gray = result.get()\n",
    "\n",
    "        if detected:\n",
    "            current_abnormality_time = frame_number / original_fps\n",
    "\n",
    "            if not abnormality_detected:\n",
    "                if last_abnormality_time is None or (current_abnormality_time - last_abnormality_time) >= (min_abnormality_gap_minutes * 60):\n",
    "                    abnormality_group_id += 1\n",
    "\n",
    "                abnormality_start_frame = frame_number\n",
    "                abnormality_detected = True\n",
    "                abnormality_times.append(frame_number)\n",
    "                last_abnormality_time = current_abnormality_time\n",
    "\n",
    "        else:\n",
    "            if abnormality_detected:\n",
    "                abnormality_detected = False\n",
    "                abnormality_end_frame = frame_number\n",
    "                abnormality_start_seconds = abnormality_start_frame / original_fps\n",
    "                abnormality_end_seconds = abnormality_end_frame / original_fps\n",
    "                abnormality_duration_seconds = abnormality_end_seconds - abnormality_start_seconds\n",
    "\n",
    "                print(f\"Abnormality {abnormality_group_id} start time: {int(abnormality_start_seconds // 60)} minutes {abnormality_start_seconds % 60:.2f} seconds\")\n",
    "                print(f\"Abnormality {abnormality_group_id} duration: {int(abnormality_duration_seconds // 60)} minutes {abnormality_duration_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    cap.release()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    total_time_seconds = frame_number / original_fps\n",
    "    print(f\"Total video time: {int(total_time_seconds // 60)} minutes {total_time_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    return abnormality_times\n",
    "\n",
    "# Example usage\n",
    "video_path = r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\output48.mp4\"\n",
    "output_dir = 'C:/Users/pkathi/Desktop/my work/computervision-video/augment2_1'\n",
    "abnormality_times = motion_detection(video_path, output_dir, minimal_fps=5)\n",
    "\n",
    "if abnormality_times:\n",
    "    print(\"Abnormality detected at frame numbers:\", abnormality_times)\n",
    "else:\n",
    "    print(\"No abnormality detected in the video.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Augmentation and Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''VGG16'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 228 images belonging to 2 classes.\n",
      "Found 56 images belonging to 2 classes.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkathi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 4s/step - accuracy: 0.5524 - loss: 0.9504 - val_accuracy: 0.4821 - val_loss: 1.0380\n",
      "Epoch 2/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3s/step - accuracy: 0.5102 - loss: 0.7178 - val_accuracy: 0.6607 - val_loss: 0.5400\n",
      "Epoch 3/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 4s/step - accuracy: 0.7161 - loss: 0.7091 - val_accuracy: 0.4821 - val_loss: 0.7313\n",
      "Epoch 4/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 4s/step - accuracy: 0.6420 - loss: 0.5366 - val_accuracy: 0.5000 - val_loss: 0.6765\n",
      "Epoch 5/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3s/step - accuracy: 0.7755 - loss: 0.4529 - val_accuracy: 0.5893 - val_loss: 0.5434\n",
      "Epoch 6/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3s/step - accuracy: 0.7784 - loss: 0.4527 - val_accuracy: 0.5536 - val_loss: 0.6962\n",
      "Epoch 7/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3s/step - accuracy: 0.7656 - loss: 0.4279 - val_accuracy: 0.5893 - val_loss: 0.6002\n",
      "Epoch 8/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3s/step - accuracy: 0.7911 - loss: 0.4069 - val_accuracy: 0.5536 - val_loss: 0.8482\n",
      "Epoch 9/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3s/step - accuracy: 0.8020 - loss: 0.4359 - val_accuracy: 0.5536 - val_loss: 0.7812\n",
      "Epoch 10/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3s/step - accuracy: 0.8312 - loss: 0.3988 - val_accuracy: 0.4821 - val_loss: 1.2056\n",
      "Epoch 11/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3s/step - accuracy: 0.8028 - loss: 0.3597 - val_accuracy: 0.4821 - val_loss: 1.1558\n",
      "Epoch 12/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3s/step - accuracy: 0.8855 - loss: 0.3160 - val_accuracy: 0.5536 - val_loss: 0.8969\n",
      "Epoch 13/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3s/step - accuracy: 0.8544 - loss: 0.3109 - val_accuracy: 0.5536 - val_loss: 1.1515\n",
      "Epoch 14/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3s/step - accuracy: 0.8138 - loss: 0.3251 - val_accuracy: 0.5536 - val_loss: 0.9398\n",
      "Epoch 15/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3s/step - accuracy: 0.8622 - loss: 0.3482 - val_accuracy: 0.4821 - val_loss: 1.3266\n",
      "Epoch 16/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - accuracy: 0.8351 - loss: 0.3509 - val_accuracy: 0.5536 - val_loss: 1.0672\n",
      "Epoch 17/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - accuracy: 0.8192 - loss: 0.3470 - val_accuracy: 0.5536 - val_loss: 1.1383\n",
      "Epoch 18/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3s/step - accuracy: 0.8717 - loss: 0.3429 - val_accuracy: 0.5357 - val_loss: 1.1987\n",
      "Epoch 19/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - accuracy: 0.8757 - loss: 0.3012 - val_accuracy: 0.5536 - val_loss: 0.9908\n",
      "Epoch 20/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - accuracy: 0.9281 - loss: 0.2512 - val_accuracy: 0.5536 - val_loss: 1.1142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: {'Fire': 0, 'Smoke': 1}\n",
      "Number of training samples: 228\n",
      "Number of validation samples: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470ms/step\n",
      "Fire detected at 0m_45.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
      "Fire detected at 0m_45.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "Fire detected at 0m_45.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step\n",
      "Fire detected at 0m_45.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step\n",
      "Smoke detected at 0m_45.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step\n",
      "Fire detected at 0m_46.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
      "Fire detected at 0m_46.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step\n",
      "Fire detected at 0m_46.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n",
      "Fire detected at 0m_46.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "Fire detected at 0m_46.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "Fire detected at 0m_47.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
      "Fire detected at 0m_47.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n",
      "Fire detected at 0m_47.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step\n",
      "Fire detected at 0m_47.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "Fire detected at 0m_47.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n",
      "Fire detected at 0m_48.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
      "Fire detected at 0m_48.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
      "Fire detected at 0m_48.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "Fire detected at 0m_48.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step\n",
      "Fire detected at 0m_48.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "Fire detected at 0m_49.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
      "Fire detected at 0m_49.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
      "Fire detected at 0m_49.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "Fire detected at 0m_49.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
      "Fire detected at 0m_49.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "Fire detected at 0m_50.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "Fire detected at 0m_50.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
      "Fire detected at 0m_50.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "Fire detected at 0m_50.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step\n",
      "Fire detected at 0m_50.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "Fire detected at 0m_51.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step\n",
      "Fire detected at 0m_51.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "Fire detected at 0m_51.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
      "Fire detected at 0m_51.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
      "Fire detected at 0m_51.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step\n",
      "Fire detected at 0m_52.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "Fire detected at 0m_52.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
      "Fire detected at 0m_52.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
      "Fire detected at 0m_52.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
      "Fire detected at 0m_52.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
      "Fire detected at 0m_53.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n",
      "Fire detected at 0m_53.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step\n",
      "Fire detected at 0m_53.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "Fire detected at 0m_53.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "Fire detected at 0m_53.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
      "Fire detected at 0m_54.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n",
      "Fire detected at 0m_54.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step\n",
      "Fire detected at 0m_54.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
      "Fire detected at 0m_54.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
      "Fire detected at 0m_54.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step\n",
      "Fire detected at 0m_55.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step\n",
      "Fire detected at 0m_55.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "Fire detected at 0m_55.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
      "Fire detected at 0m_55.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n",
      "Fire detected at 0m_55.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
      "Fire detected at 0m_56.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step\n",
      "Fire detected at 0m_56.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step\n",
      "Fire detected at 0m_56.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "Fire detected at 0m_56.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "Fire detected at 0m_56.80s\n",
      "Abnormality 1 start time: 0 minutes 45.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 12.00 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
      "Fire detected at 0m_57.40s\n",
      "Abnormality 1 start time: 0 minutes 57.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n",
      "Fire detected at 0m_57.80s\n",
      "Abnormality 1 start time: 0 minutes 57.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "Fire detected at 0m_58.20s\n",
      "Abnormality 1 start time: 0 minutes 58.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step\n",
      "Fire detected at 0m_58.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "Fire detected at 0m_58.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step\n",
      "Fire detected at 0m_59.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "Fire detected at 0m_59.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step\n",
      "Fire detected at 0m_59.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
      "Fire detected at 0m_59.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
      "Fire detected at 0m_59.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step\n",
      "Fire detected at 0m_60.00s\n",
      "Total video time: 0 minutes 60.00 seconds\n",
      "Abnormality detected at frame numbers: [1350, 1722, 1734, 1746, 1758]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Load and preprocess dataset with data augmentation\n",
    "def prepare_dataset(data_dir, target_size=(128, 128), batch_size=32):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255.0,\n",
    "        validation_split=0.2,\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    validation_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255.0,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Define the model using transfer learning (VGG16)\n",
    "def create_transfer_learning_model(input_shape, num_classes):\n",
    "    base_model = VGG16(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model with class weighting\n",
    "def train_model(model, train_generator, validation_generator, epochs=20):\n",
    "    # Extract class indices from the training generator\n",
    "    class_indices = {v: k for k, v in train_generator.class_indices.items()}\n",
    "    classes = np.array([class_indices[i] for i in train_generator.classes])\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
    "    class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        class_weight=class_weights_dict\n",
    "    )\n",
    "\n",
    "# Use the model to classify abnormalities\n",
    "def classify_abnormality(model, frame):\n",
    "    frame_resized = cv2.resize(frame, (128, 128))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    classes = ['Fire', 'Smoke']\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "# Example usage\n",
    "data_dir = r'C:\\Users\\pkathi\\Desktop\\object detection\\tradianal approach-cv\\Dataset2'\n",
    "input_shape = (128, 128, 3)\n",
    "num_classes = 2\n",
    "\n",
    "train_gen, val_gen = prepare_dataset(data_dir, target_size=(128, 128))\n",
    "cnn_model = create_transfer_learning_model(input_shape, num_classes)\n",
    "train_model(cnn_model, train_gen, val_gen)\n",
    "\n",
    "# Save the trained model\n",
    "cnn_model.save('abnormality_classifier_vgg16.h5')\n",
    "\n",
    "# Verify the dataset preparation\n",
    "print(f\"Classes found: {train_gen.class_indices}\")\n",
    "print(f\"Number of training samples: {train_gen.samples}\")\n",
    "print(f\"Number of validation samples: {val_gen.samples}\")\n",
    "\n",
    "# Now use the saved model for classifying abnormalities in video frames\n",
    "cnn_model = load_model('abnormality_classifier_vgg16.h5')\n",
    "\n",
    "# Update the process_frame function to include classification and saving frames\n",
    "def process_frame(frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir):\n",
    "    frame_resized = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    frame_diff = cv2.absdiff(prev_frame_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = any(cv2.contourArea(contour) > min_contour_area for contour in contours)\n",
    "    if detected:\n",
    "        # Classify the abnormality\n",
    "        classification = classify_abnormality(cnn_model, frame_resized)\n",
    "        \n",
    "        abnormality_time = frame_number / original_fps\n",
    "        timestamp = f\"{int(abnormality_time // 60)}m_{abnormality_time % 60:.2f}s\"\n",
    "        output_frame_path = os.path.join(output_dir, classification, f\"{classification}_{timestamp}_frame_{frame_number}.jpg\")\n",
    "\n",
    "        os.makedirs(os.path.dirname(output_frame_path), exist_ok=True)\n",
    "        cv2.imwrite(output_frame_path, frame_resized)\n",
    "\n",
    "        print(f\"{classification} detected at {timestamp}\")\n",
    "    \n",
    "    return detected, frame_gray\n",
    "\n",
    "# Motion detection function remains the same\n",
    "def motion_detection(video_path, output_dir, threshold_value=25, min_contour_area=500, resize_factor=0.5, minimal_fps=5, min_abnormality_gap_minutes=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip_interval = int(round(original_fps / minimal_fps))\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame\")\n",
    "        return\n",
    "\n",
    "    prev_frame = cv2.resize(prev_frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    abnormality_times = []\n",
    "\n",
    "    frame_number = 0\n",
    "    abnormality_detected = False\n",
    "    abnormality_start_frame = None\n",
    "    abnormality_group_id = 0\n",
    "    last_abnormality_time = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pool = ThreadPool(processes=4)  # Use a thread pool with 4 threads\n",
    "\n",
    "    while True:\n",
    "        for _ in range(frame_skip_interval):\n",
    "            ret = cap.grab()  # Use grab to skip frames efficiently\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_number += 1\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.retrieve()  # Retrieve the frame after skipping\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use thread pool to process frame\n",
    "        result = pool.apply_async(process_frame, (frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir))\n",
    "        detected, prev_frame_gray = result.get()\n",
    "\n",
    "        if detected:\n",
    "            current_abnormality_time = frame_number / original_fps\n",
    "\n",
    "            if not abnormality_detected:\n",
    "                if last_abnormality_time is None or (current_abnormality_time - last_abnormality_time) >= (min_abnormality_gap_minutes * 60):\n",
    "                    abnormality_group_id += 1\n",
    "\n",
    "                abnormality_start_frame = frame_number\n",
    "                abnormality_detected = True\n",
    "                abnormality_times.append(frame_number)\n",
    "                last_abnormality_time = current_abnormality_time\n",
    "\n",
    "        else:\n",
    "            if abnormality_detected:\n",
    "                abnormality_end_time = frame_number / original_fps\n",
    "                abnormality_duration_seconds = abnormality_end_time - (abnormality_start_frame / original_fps)\n",
    "\n",
    "                print(f\"Abnormality {abnormality_group_id} start time: {int((abnormality_start_frame / original_fps) // 60)} minutes {(abnormality_start_frame / original_fps) % 60:.2f} seconds\")\n",
    "                print(f\"Abnormality {abnormality_group_id} duration: {int(abnormality_duration_seconds // 60)} minutes {abnormality_duration_seconds % 60:.2f} seconds\")\n",
    "\n",
    "                abnormality_detected = False\n",
    "\n",
    "    cap.release()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    total_time_seconds = frame_number / original_fps\n",
    "    print(f\"Total video time: {int(total_time_seconds // 60)} minutes {total_time_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    return abnormality_times\n",
    "\n",
    "# Example usage\n",
    "video_path = r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\output48.mp4\"\n",
    "output_dir = 'C:/Users/pkathi/Desktop/my work/computervision-video/augmentvgg_0'\n",
    "abnormality_times = motion_detection(video_path, output_dir, minimal_fps=5)\n",
    "\n",
    "if abnormality_times:\n",
    "    print(\"Abnormality detected at frame numbers:\", abnormality_times)\n",
    "else:\n",
    "    print(\"No abnormality detected in the video.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 228 images belonging to 2 classes.\n",
      "Found 56 images belonging to 2 classes.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkathi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 4s/step - accuracy: 0.5240 - loss: 0.7980 - val_accuracy: 0.1429 - val_loss: 0.9788\n",
      "Epoch 2/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 4s/step - accuracy: 0.4831 - loss: 0.7567 - val_accuracy: 0.4821 - val_loss: 0.6176\n",
      "Epoch 3/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 4s/step - accuracy: 0.6892 - loss: 0.5706 - val_accuracy: 0.5000 - val_loss: 0.6542\n",
      "Epoch 4/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 4s/step - accuracy: 0.6394 - loss: 0.6539 - val_accuracy: 0.5536 - val_loss: 0.6552\n",
      "Epoch 5/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3s/step - accuracy: 0.7012 - loss: 0.5873 - val_accuracy: 0.5536 - val_loss: 0.6164\n",
      "Epoch 6/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3s/step - accuracy: 0.7821 - loss: 0.4184 - val_accuracy: 0.5536 - val_loss: 0.6762\n",
      "Epoch 7/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3s/step - accuracy: 0.7581 - loss: 0.4225 - val_accuracy: 0.4821 - val_loss: 0.9584\n",
      "Epoch 8/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3s/step - accuracy: 0.7641 - loss: 0.4192 - val_accuracy: 0.5536 - val_loss: 0.8043\n",
      "Epoch 9/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 3s/step - accuracy: 0.8273 - loss: 0.3821 - val_accuracy: 0.4821 - val_loss: 1.0620\n",
      "Epoch 10/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3s/step - accuracy: 0.8293 - loss: 0.3867 - val_accuracy: 0.4821 - val_loss: 1.1466\n",
      "Epoch 11/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 4s/step - accuracy: 0.7353 - loss: 0.4073 - val_accuracy: 0.5536 - val_loss: 0.7992\n",
      "Epoch 12/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 4s/step - accuracy: 0.8700 - loss: 0.3169 - val_accuracy: 0.4821 - val_loss: 1.3367\n",
      "Epoch 13/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 3s/step - accuracy: 0.7443 - loss: 0.4060 - val_accuracy: 0.5536 - val_loss: 0.8968\n",
      "Epoch 14/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3s/step - accuracy: 0.8912 - loss: 0.3084 - val_accuracy: 0.5714 - val_loss: 0.9100\n",
      "Epoch 15/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3s/step - accuracy: 0.7972 - loss: 0.3716 - val_accuracy: 0.5536 - val_loss: 1.0527\n",
      "Epoch 16/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3s/step - accuracy: 0.8737 - loss: 0.3447 - val_accuracy: 0.5536 - val_loss: 1.0181\n",
      "Epoch 17/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3s/step - accuracy: 0.8531 - loss: 0.3055 - val_accuracy: 0.5536 - val_loss: 1.0177\n",
      "Epoch 18/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3s/step - accuracy: 0.8689 - loss: 0.3234 - val_accuracy: 0.5536 - val_loss: 1.2158\n",
      "Epoch 19/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 3s/step - accuracy: 0.8755 - loss: 0.2997 - val_accuracy: 0.5536 - val_loss: 1.0155\n",
      "Epoch 20/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 4s/step - accuracy: 0.8643 - loss: 0.3571 - val_accuracy: 0.5000 - val_loss: 1.4446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: {'Fire': 0, 'Smoke': 1}\n",
      "Number of training samples: 228\n",
      "Number of validation samples: 56\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess dataset with data augmentation\n",
    "def prepare_dataset(data_dir, target_size=(128, 128), batch_size=32):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255.0,\n",
    "        validation_split=0.2,\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    validation_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255.0,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Define the model using transfer learning (VGG16)\n",
    "def create_transfer_learning_model(input_shape, num_classes):\n",
    "    base_model = VGG16(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model with class weighting\n",
    "def train_model(model, train_generator, validation_generator, epochs=20):\n",
    "    # Extract class indices from the training generator\n",
    "    class_indices = {v: k for k, v in train_generator.class_indices.items()}\n",
    "    classes = np.array([class_indices[i] for i in train_generator.classes])\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
    "    class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        class_weight=class_weights_dict\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "data_dir = r'C:\\Users\\pkathi\\Desktop\\object detection\\tradianal approach-cv\\Dataset2'\n",
    "input_shape = (128, 128, 3)\n",
    "num_classes = 2\n",
    "\n",
    "train_gen, val_gen = prepare_dataset(data_dir, target_size=(128, 128))\n",
    "cnn_model = create_transfer_learning_model(input_shape, num_classes)\n",
    "train_model(cnn_model, train_gen, val_gen)\n",
    "\n",
    "# Save the trained model\n",
    "cnn_model.save('abnormality_classifier_vgg16.h5')\n",
    "\n",
    "# Verify the dataset preparation\n",
    "print(f\"Classes found: {train_gen.class_indices}\")\n",
    "print(f\"Number of training samples: {train_gen.samples}\")\n",
    "print(f\"Number of validation samples: {val_gen.samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464ms/step\n",
      "Smoke detected at 0m_51.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
      "Smoke detected at 0m_51.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step\n",
      "Smoke detected at 0m_51.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step\n",
      "Smoke detected at 0m_51.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step\n",
      "Smoke detected at 0m_51.80s\n",
      "Abnormality 1 start time: 0 minutes 51.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 1.00 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step\n",
      "Smoke detected at 0m_52.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step\n",
      "Smoke detected at 0m_53.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step\n",
      "Smoke detected at 0m_53.20s\n",
      "Abnormality 1 start time: 0 minutes 52.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.60 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step\n",
      "Smoke detected at 0m_55.20s\n",
      "Abnormality 1 start time: 0 minutes 55.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step\n",
      "Smoke detected at 0m_56.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 429ms/step\n",
      "Smoke detected at 0m_56.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step\n",
      "Smoke detected at 0m_56.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "Smoke detected at 0m_56.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376ms/step\n",
      "Smoke detected at 0m_57.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step\n",
      "Smoke detected at 0m_57.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step\n",
      "Smoke detected at 0m_57.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step\n",
      "Smoke detected at 0m_57.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step\n",
      "Smoke detected at 0m_57.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338ms/step\n",
      "Smoke detected at 0m_58.00s\n",
      "Abnormality 1 start time: 0 minutes 56.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 2.00 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step\n",
      "Smoke detected at 0m_58.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step\n",
      "Smoke detected at 0m_58.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step\n",
      "Smoke detected at 0m_58.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step\n",
      "Smoke detected at 0m_59.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step\n",
      "Smoke detected at 0m_59.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373ms/step\n",
      "Smoke detected at 0m_59.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 520ms/step\n",
      "Smoke detected at 0m_59.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338ms/step\n",
      "Smoke detected at 0m_59.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 449ms/step\n",
      "Smoke detected at 0m_60.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 534ms/step\n",
      "Smoke detected at 1m_0.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431ms/step\n",
      "Smoke detected at 1m_0.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step\n",
      "Smoke detected at 1m_0.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366ms/step\n",
      "Smoke detected at 1m_0.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step\n",
      "Smoke detected at 1m_1.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step\n",
      "Smoke detected at 1m_1.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step\n",
      "Smoke detected at 1m_1.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step\n",
      "Smoke detected at 1m_1.60s\n",
      "Abnormality 1 start time: 0 minutes 58.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 3.40 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step\n",
      "Smoke detected at 1m_2.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step\n",
      "Smoke detected at 1m_2.20s\n",
      "Abnormality 1 start time: 1 minutes 2.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.40 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step\n",
      "Smoke detected at 1m_2.80s\n",
      "Abnormality 1 start time: 1 minutes 2.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step\n",
      "Smoke detected at 1m_7.20s\n",
      "Abnormality 1 start time: 1 minutes 7.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step\n",
      "Smoke detected at 1m_7.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
      "Smoke detected at 1m_7.80s\n",
      "Abnormality 1 start time: 1 minutes 7.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.40 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step\n",
      "Smoke detected at 1m_8.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step\n",
      "Smoke detected at 1m_9.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step\n",
      "Smoke detected at 1m_9.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step\n",
      "Smoke detected at 1m_9.40s\n",
      "Abnormality 1 start time: 1 minutes 8.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.80 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
      "Smoke detected at 1m_10.80s\n",
      "Abnormality 1 start time: 1 minutes 10.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step\n",
      "Smoke detected at 1m_12.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step\n",
      "Smoke detected at 1m_12.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384ms/step\n",
      "Smoke detected at 1m_12.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step\n",
      "Smoke detected at 1m_13.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step\n",
      "Smoke detected at 1m_13.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step\n",
      "Smoke detected at 1m_13.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step\n",
      "Smoke detected at 1m_13.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step\n",
      "Smoke detected at 1m_13.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step\n",
      "Smoke detected at 1m_14.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step\n",
      "Smoke detected at 1m_14.20s\n",
      "Abnormality 1 start time: 1 minutes 12.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 2.00 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step\n",
      "Smoke detected at 1m_15.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step\n",
      "Smoke detected at 1m_15.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step\n",
      "Smoke detected at 1m_15.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step\n",
      "Smoke detected at 1m_15.60s\n",
      "Abnormality 1 start time: 1 minutes 15.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.80 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step\n",
      "Smoke detected at 1m_16.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step\n",
      "Smoke detected at 1m_17.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step\n",
      "Smoke detected at 1m_17.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step\n",
      "Smoke detected at 1m_17.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step\n",
      "Smoke detected at 1m_17.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step\n",
      "Smoke detected at 1m_17.80s\n",
      "Abnormality 1 start time: 1 minutes 16.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 1.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step\n",
      "Smoke detected at 1m_18.40s\n",
      "Abnormality 1 start time: 1 minutes 18.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step\n",
      "Smoke detected at 1m_19.20s\n",
      "Abnormality 1 start time: 1 minutes 19.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "Total video time: 1 minutes 60.00 seconds\n",
      "Abnormality frames saved in the output directory.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Use the model to classify abnormalities\n",
    "def classify_abnormality(model, frame):\n",
    "    frame_resized = cv2.resize(frame, (128, 128))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    classes = ['Fire', 'Smoke']\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "# Update the process_frame function to include classification and saving frames\n",
    "def process_frame(frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, cnn_model):\n",
    "    frame_resized = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    frame_diff = cv2.absdiff(prev_frame_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = any(cv2.contourArea(contour) > min_contour_area for contour in contours)\n",
    "    if detected:\n",
    "        # Classify the abnormality\n",
    "        classification = classify_abnormality(cnn_model, frame_resized)\n",
    "        \n",
    "        abnormality_time = frame_number / original_fps\n",
    "        timestamp = f\"{int(abnormality_time // 60)}m_{abnormality_time % 60:.2f}s\"\n",
    "        output_frame_path = os.path.join(output_dir, classification, f\"{classification}_{timestamp}_frame_{frame_number}.jpg\")\n",
    "\n",
    "        os.makedirs(os.path.dirname(output_frame_path), exist_ok=True)\n",
    "        cv2.imwrite(output_frame_path, frame_resized)\n",
    "\n",
    "        print(f\"{classification} detected at {timestamp}\")\n",
    "    \n",
    "    return detected, frame_gray\n",
    "\n",
    "# Motion detection function\n",
    "def motion_detection(video_path, output_dir, cnn_model, threshold_value=25, min_contour_area=500, resize_factor=0.5, minimal_fps=5, min_abnormality_gap_minutes=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip_interval = int(round(original_fps / minimal_fps))\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame\")\n",
    "        return\n",
    "\n",
    "    prev_frame = cv2.resize(prev_frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    abnormality_times = []\n",
    "\n",
    "    frame_number = 0\n",
    "    abnormality_detected = False\n",
    "    abnormality_start_frame = None\n",
    "    abnormality_group_id = 0\n",
    "    last_abnormality_time = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pool = ThreadPool(processes=4)  # Use a thread pool with 4 threads\n",
    "\n",
    "    while True:\n",
    "        for _ in range(frame_skip_interval):\n",
    "            ret = cap.grab()  # Use grab to skip frames efficiently\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_number += 1\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.retrieve()  # Retrieve the frame after skipping\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use thread pool to process frame\n",
    "        result = pool.apply_async(process_frame, (frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, cnn_model))\n",
    "        detected, prev_frame_gray = result.get()\n",
    "\n",
    "        if detected:\n",
    "            current_abnormality_time = frame_number / original_fps\n",
    "\n",
    "            if not abnormality_detected:\n",
    "                if last_abnormality_time is None or (current_abnormality_time - last_abnormality_time) >= (min_abnormality_gap_minutes * 60):\n",
    "                    abnormality_group_id += 1\n",
    "\n",
    "                abnormality_start_frame = frame_number\n",
    "                abnormality_detected = True\n",
    "                abnormality_times.append(frame_number)\n",
    "                last_abnormality_time = current_abnormality_time\n",
    "\n",
    "        else:\n",
    "            if abnormality_detected:\n",
    "                abnormality_end_time = frame_number / original_fps\n",
    "                abnormality_duration_seconds = abnormality_end_time - (abnormality_start_frame / original_fps)\n",
    "\n",
    "                print(f\"Abnormality {abnormality_group_id} start time: {int((abnormality_start_frame / original_fps) // 60)} minutes {(abnormality_start_frame / original_fps) % 60:.2f} seconds\")\n",
    "                print(f\"Abnormality {abnormality_group_id} duration: {int(abnormality_duration_seconds // 60)} minutes {abnormality_duration_seconds % 60:.2f} seconds\")\n",
    "\n",
    "                abnormality_detected = False\n",
    "\n",
    "    cap.release()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    total_time_seconds = frame_number / original_fps\n",
    "    print(f\"Total video time: {int(total_time_seconds // 60)} minutes {total_time_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    return abnormality_times\n",
    "\n",
    "# Load the trained model\n",
    "cnn_model = load_model('abnormality_classifier_vgg16.h5')\n",
    "\n",
    "# Example usage\n",
    "video_path =  r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\2-DMG_RSA_HTE101_D_HS_FIRE_31-33min\\output31.mp4\"\n",
    "output_dir = r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\augmentvgg_1\"\n",
    "abnormality_times = motion_detection(video_path, output_dir, cnn_model)\n",
    "print(\"Abnormality frames saved in the output directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56 images belonging to 2 classes.\n",
      "Found 13 images belonging to 2 classes.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkathi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.5893 - loss: 0.6971 - val_accuracy: 0.6154 - val_loss: 0.8226\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.4077 - loss: 0.9131 - val_accuracy: 0.6154 - val_loss: 0.7983\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.6131 - loss: 0.6896 - val_accuracy: 0.6154 - val_loss: 0.7609\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.6711 - loss: 0.6860 - val_accuracy: 0.6154 - val_loss: 0.7318\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.5893 - loss: 0.6986 - val_accuracy: 0.6154 - val_loss: 0.7414\n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.7277 - loss: 0.5306 - val_accuracy: 0.6154 - val_loss: 0.7049\n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.6577 - loss: 0.7210 - val_accuracy: 0.6154 - val_loss: 0.6566\n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.6865 - loss: 0.6056 - val_accuracy: 0.6154 - val_loss: 0.6291\n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.7599 - loss: 0.4736 - val_accuracy: 0.4615 - val_loss: 0.6064\n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.7004 - loss: 0.5322 - val_accuracy: 0.4615 - val_loss: 0.5989\n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.7361 - loss: 0.5345 - val_accuracy: 0.4615 - val_loss: 0.5985\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.7723 - loss: 0.5048 - val_accuracy: 0.6154 - val_loss: 0.6096\n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.7396 - loss: 0.5404 - val_accuracy: 0.6154 - val_loss: 0.6242\n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.7827 - loss: 0.4851 - val_accuracy: 0.4615 - val_loss: 0.6115\n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.8065 - loss: 0.4124 - val_accuracy: 0.4615 - val_loss: 0.6153\n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.8492 - loss: 0.4018 - val_accuracy: 0.4615 - val_loss: 0.6080\n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step - accuracy: 0.7381 - loss: 0.5645 - val_accuracy: 0.4615 - val_loss: 0.6068\n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.7857 - loss: 0.5053 - val_accuracy: 0.4615 - val_loss: 0.6145\n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.7500 - loss: 0.4381 - val_accuracy: 0.6154 - val_loss: 0.6236\n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.8373 - loss: 0.4396 - val_accuracy: 0.6154 - val_loss: 0.6516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: {'Fire': 0, 'Smoke': 1}\n",
      "Number of training samples: 56\n",
      "Number of validation samples: 13\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess dataset with data augmentation\n",
    "def prepare_dataset(data_dir, target_size=(128, 128), batch_size=32):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255.0,\n",
    "        validation_split=0.2,\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    validation_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255.0,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Define the model using transfer learning (VGG16)\n",
    "def create_transfer_learning_model(input_shape, num_classes):\n",
    "    base_model = VGG16(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model with class weighting\n",
    "def train_model(model, train_generator, validation_generator, epochs=20):\n",
    "    # Extract class indices from the training generator\n",
    "    class_indices = {v: k for k, v in train_generator.class_indices.items()}\n",
    "    classes = np.array([class_indices[i] for i in train_generator.classes])\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
    "    class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        class_weight=class_weights_dict\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "data_dir = r'C:\\Users\\pkathi\\Desktop\\object detection\\tradianal approach-cv\\Dataset2-minimal'\n",
    "input_shape = (128, 128, 3)\n",
    "num_classes = 2\n",
    "\n",
    "train_gen, val_gen = prepare_dataset(data_dir, target_size=(128, 128))\n",
    "cnn_model = create_transfer_learning_model(input_shape, num_classes)\n",
    "train_model(cnn_model, train_gen, val_gen)\n",
    "\n",
    "# Save the trained model\n",
    "cnn_model.save('abnormality_classifier_vgg16.h5')\n",
    "\n",
    "# Verify the dataset preparation\n",
    "print(f\"Classes found: {train_gen.class_indices}\")\n",
    "print(f\"Number of training samples: {train_gen.samples}\")\n",
    "print(f\"Number of validation samples: {val_gen.samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step\n",
      "Smoke detected at 0m_57.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Smoke detected at 0m_57.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "Smoke detected at 0m_58.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "Smoke detected at 0m_58.20s\n",
      "Abnormality 1 start time: 0 minutes 57.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.80 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "Smoke detected at 0m_58.60s\n",
      "Abnormality 1 start time: 0 minutes 58.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "Fire detected at 1m_3.00s\n",
      "Abnormality 1 start time: 1 minutes 3.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Fire detected at 1m_3.40s\n",
      "Abnormality 1 start time: 1 minutes 3.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
      "Smoke detected at 1m_17.00s\n",
      "Abnormality 1 start time: 1 minutes 17.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step\n",
      "Smoke detected at 1m_17.40s\n",
      "Abnormality 1 start time: 1 minutes 17.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "Total video time: 1 minutes 60.00 seconds\n",
      "Abnormality frames saved in the output directory.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Use the model to classify abnormalities\n",
    "def classify_abnormality(model, frame):\n",
    "    frame_resized = cv2.resize(frame, (128, 128))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    classes = ['Fire', 'Smoke']\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "# Update the process_frame function to include classification and saving frames\n",
    "def process_frame(frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, cnn_model):\n",
    "    frame_resized = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    frame_diff = cv2.absdiff(prev_frame_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = any(cv2.contourArea(contour) > min_contour_area for contour in contours)\n",
    "    if detected:\n",
    "        # Classify the abnormality\n",
    "        classification = classify_abnormality(cnn_model, frame_resized)\n",
    "        \n",
    "        abnormality_time = frame_number / original_fps\n",
    "        timestamp = f\"{int(abnormality_time // 60)}m_{abnormality_time % 60:.2f}s\"\n",
    "        output_frame_path = os.path.join(output_dir, classification, f\"{classification}_{timestamp}_frame_{frame_number}.jpg\")\n",
    "\n",
    "        os.makedirs(os.path.dirname(output_frame_path), exist_ok=True)\n",
    "        cv2.imwrite(output_frame_path, frame_resized)\n",
    "\n",
    "        print(f\"{classification} detected at {timestamp}\")\n",
    "    \n",
    "    return detected, frame_gray\n",
    "\n",
    "# Motion detection function\n",
    "def motion_detection(video_path, output_dir, cnn_model, threshold_value=25, min_contour_area=500, resize_factor=0.5, minimal_fps=5, min_abnormality_gap_minutes=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip_interval = int(round(original_fps / minimal_fps))\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame\")\n",
    "        return\n",
    "\n",
    "    prev_frame = cv2.resize(prev_frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    abnormality_times = []\n",
    "\n",
    "    frame_number = 0\n",
    "    abnormality_detected = False\n",
    "    abnormality_start_frame = None\n",
    "    abnormality_group_id = 0\n",
    "    last_abnormality_time = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pool = ThreadPool(processes=4)  # Use a thread pool with 4 threads\n",
    "\n",
    "    while True:\n",
    "        for _ in range(frame_skip_interval):\n",
    "            ret = cap.grab()  # Use grab to skip frames efficiently\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_number += 1\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.retrieve()  # Retrieve the frame after skipping\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use thread pool to process frame\n",
    "        result = pool.apply_async(process_frame, (frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, cnn_model))\n",
    "        detected, prev_frame_gray = result.get()\n",
    "\n",
    "        if detected:\n",
    "            current_abnormality_time = frame_number / original_fps\n",
    "\n",
    "            if not abnormality_detected:\n",
    "                if last_abnormality_time is None or (current_abnormality_time - last_abnormality_time) >= (min_abnormality_gap_minutes * 60):\n",
    "                    abnormality_group_id += 1\n",
    "\n",
    "                abnormality_start_frame = frame_number\n",
    "                abnormality_detected = True\n",
    "                abnormality_times.append(frame_number)\n",
    "                last_abnormality_time = current_abnormality_time\n",
    "\n",
    "        else:\n",
    "            if abnormality_detected:\n",
    "                abnormality_end_time = frame_number / original_fps\n",
    "                abnormality_duration_seconds = abnormality_end_time - (abnormality_start_frame / original_fps)\n",
    "\n",
    "                print(f\"Abnormality {abnormality_group_id} start time: {int((abnormality_start_frame / original_fps) // 60)} minutes {(abnormality_start_frame / original_fps) % 60:.2f} seconds\")\n",
    "                print(f\"Abnormality {abnormality_group_id} duration: {int(abnormality_duration_seconds // 60)} minutes {abnormality_duration_seconds % 60:.2f} seconds\")\n",
    "\n",
    "                abnormality_detected = False\n",
    "\n",
    "    cap.release()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    total_time_seconds = frame_number / original_fps\n",
    "    print(f\"Total video time: {int(total_time_seconds // 60)} minutes {total_time_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    return abnormality_times\n",
    "\n",
    "# Load the trained model\n",
    "cnn_model = load_model('abnormality_classifier_vgg16.h5')\n",
    "\n",
    "# Example usage\n",
    "video_path =  r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\C1007trimmed116-118.mp4\"\n",
    "output_dir = r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\augmentvgg_3minima\"\n",
    "abnormality_times = motion_detection(video_path, output_dir, cnn_model)\n",
    "print(\"Abnormality frames saved in the output directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46 images belonging to 2 classes.\n",
      "Found 10 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkathi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.4896 - loss: 0.9173 - val_accuracy: 0.8000 - val_loss: 0.6807\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.6491 - loss: 0.7740 - val_accuracy: 0.8000 - val_loss: 0.6683\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.5673 - loss: 0.8687 - val_accuracy: 0.2000 - val_loss: 0.9383\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.4037 - loss: 1.1092 - val_accuracy: 0.2000 - val_loss: 1.3336\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.4420 - loss: 0.8384 - val_accuracy: 0.2000 - val_loss: 1.0675\n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.4576 - loss: 0.6961 - val_accuracy: 0.4000 - val_loss: 0.6322\n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.7164 - loss: 0.5985 - val_accuracy: 1.0000 - val_loss: 0.3653\n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.8320 - loss: 0.3998 - val_accuracy: 1.0000 - val_loss: 0.2819\n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.7836 - loss: 0.5811 - val_accuracy: 1.0000 - val_loss: 0.3092\n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.7692 - loss: 0.7691 - val_accuracy: 1.0000 - val_loss: 0.4504\n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.8219 - loss: 0.3670 - val_accuracy: 0.2000 - val_loss: 0.6084\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.6584 - loss: 0.5748 - val_accuracy: 0.2000 - val_loss: 0.7230\n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.6744 - loss: 0.4807 - val_accuracy: 0.2000 - val_loss: 0.6564\n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.6449 - loss: 0.5673 - val_accuracy: 1.0000 - val_loss: 0.4187\n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.8126 - loss: 0.4097 - val_accuracy: 1.0000 - val_loss: 0.2904\n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.7453 - loss: 0.6258 - val_accuracy: 1.0000 - val_loss: 0.2825\n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.8892 - loss: 0.4299 - val_accuracy: 1.0000 - val_loss: 0.3308\n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.7428 - loss: 0.5060 - val_accuracy: 1.0000 - val_loss: 0.3855\n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.8219 - loss: 0.3589 - val_accuracy: 1.0000 - val_loss: 0.3510\n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.8271 - loss: 0.3260 - val_accuracy: 1.0000 - val_loss: 0.2639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: {'Fire': 0, 'Smoke': 1}\n",
      "Number of training samples: 46\n",
      "Number of validation samples: 10\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess dataset with data augmentation\n",
    "def prepare_dataset(data_dir, target_size=(128, 128), batch_size=32):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255.0,\n",
    "        validation_split=0.2,\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    validation_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255.0,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Define the model using transfer learning (VGG16)\n",
    "def create_transfer_learning_model(input_shape, num_classes):\n",
    "    base_model = VGG16(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model with class weighting\n",
    "def train_model(model, train_generator, validation_generator, epochs=20):\n",
    "    # Extract class indices from the training generator\n",
    "    class_indices = {v: k for k, v in train_generator.class_indices.items()}\n",
    "    classes = np.array([class_indices[i] for i in train_generator.classes])\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
    "    class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        class_weight=class_weights_dict\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "data_dir = r\"C:\\Users\\pkathi\\Desktop\\my work\\object detection\\tradianal approach-cv\\Dataset2-minimal\"\n",
    "input_shape = (128, 128, 3)\n",
    "num_classes = 2\n",
    "\n",
    "train_gen, val_gen = prepare_dataset(data_dir, target_size=(128, 128))\n",
    "cnn_model = create_transfer_learning_model(input_shape, num_classes)\n",
    "train_model(cnn_model, train_gen, val_gen)\n",
    "\n",
    "# Save the trained model\n",
    "cnn_model.save('abnormality_classifier_vgg16.h5')\n",
    "\n",
    "# Verify the dataset preparation\n",
    "print(f\"Classes found: {train_gen.class_indices}\")\n",
    "print(f\"Number of training samples: {train_gen.samples}\")\n",
    "print(f\"Number of validation samples: {val_gen.samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495ms/step\n",
      "Fire detected at 0m_57.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
      "Fire detected at 0m_57.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "Fire detected at 0m_58.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n",
      "Fire detected at 0m_58.20s\n",
      "Abnormality 1 start time: 0 minutes 57.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.80 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
      "Fire detected at 0m_58.60s\n",
      "Abnormality 1 start time: 0 minutes 58.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step\n",
      "Fire detected at 1m_3.00s\n",
      "Abnormality 1 start time: 1 minutes 3.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step\n",
      "Fire detected at 1m_3.40s\n",
      "Abnormality 1 start time: 1 minutes 3.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n",
      "Smoke detected at 1m_17.00s\n",
      "Abnormality 1 start time: 1 minutes 17.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n",
      "Fire detected at 1m_17.40s\n",
      "Abnormality 1 start time: 1 minutes 17.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "Total video time: 1 minutes 60.00 seconds\n",
      "Abnormality frames saved in the output directory.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Use the model to classify abnormalities\n",
    "def classify_abnormality(model, frame):\n",
    "    frame_resized = cv2.resize(frame, (128, 128))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    classes = ['Fire', 'Smoke']\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "# Update the process_frame function to include classification and saving frames\n",
    "def process_frame(frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, cnn_model):\n",
    "    frame_resized = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    frame_diff = cv2.absdiff(prev_frame_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = any(cv2.contourArea(contour) > min_contour_area for contour in contours)\n",
    "    if detected:\n",
    "        # Classify the abnormality\n",
    "        classification = classify_abnormality(cnn_model, frame_resized)\n",
    "        \n",
    "        abnormality_time = frame_number / original_fps\n",
    "        timestamp = f\"{int(abnormality_time // 60)}m_{abnormality_time % 60:.2f}s\"\n",
    "        output_frame_path = os.path.join(output_dir, classification, f\"{classification}_{timestamp}_frame_{frame_number}.jpg\")\n",
    "\n",
    "        os.makedirs(os.path.dirname(output_frame_path), exist_ok=True)\n",
    "        cv2.imwrite(output_frame_path, frame_resized)\n",
    "\n",
    "        print(f\"{classification} detected at {timestamp}\")\n",
    "    \n",
    "    return detected, frame_gray\n",
    "\n",
    "# Motion detection function\n",
    "def motion_detection(video_path, output_dir, cnn_model, threshold_value=25, min_contour_area=500, resize_factor=0.5, minimal_fps=5, min_abnormality_gap_minutes=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip_interval = int(round(original_fps / minimal_fps))\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame\")\n",
    "        return\n",
    "\n",
    "    prev_frame = cv2.resize(prev_frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    abnormality_times = []\n",
    "\n",
    "    frame_number = 0\n",
    "    abnormality_detected = False\n",
    "    abnormality_start_frame = None\n",
    "    abnormality_group_id = 0\n",
    "    last_abnormality_time = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pool = ThreadPool(processes=4)  # Use a thread pool with 4 threads\n",
    "\n",
    "    while True:\n",
    "        for _ in range(frame_skip_interval):\n",
    "            ret = cap.grab()  # Use grab to skip frames efficiently\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_number += 1\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.retrieve()  # Retrieve the frame after skipping\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use thread pool to process frame\n",
    "        result = pool.apply_async(process_frame, (frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, cnn_model))\n",
    "        detected, prev_frame_gray = result.get()\n",
    "\n",
    "        if detected:\n",
    "            current_abnormality_time = frame_number / original_fps\n",
    "\n",
    "            if not abnormality_detected:\n",
    "                if last_abnormality_time is None or (current_abnormality_time - last_abnormality_time) >= (min_abnormality_gap_minutes * 60):\n",
    "                    abnormality_group_id += 1\n",
    "\n",
    "                abnormality_start_frame = frame_number\n",
    "                abnormality_detected = True\n",
    "                abnormality_times.append(frame_number)\n",
    "                last_abnormality_time = current_abnormality_time\n",
    "\n",
    "        else:\n",
    "            if abnormality_detected:\n",
    "                abnormality_end_time = frame_number / original_fps\n",
    "                abnormality_duration_seconds = abnormality_end_time - (abnormality_start_frame / original_fps)\n",
    "\n",
    "                print(f\"Abnormality {abnormality_group_id} start time: {int((abnormality_start_frame / original_fps) // 60)} minutes {(abnormality_start_frame / original_fps) % 60:.2f} seconds\")\n",
    "                print(f\"Abnormality {abnormality_group_id} duration: {int(abnormality_duration_seconds // 60)} minutes {abnormality_duration_seconds % 60:.2f} seconds\")\n",
    "\n",
    "                abnormality_detected = False\n",
    "\n",
    "    cap.release()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    total_time_seconds = frame_number / original_fps\n",
    "    print(f\"Total video time: {int(total_time_seconds // 60)} minutes {total_time_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    return abnormality_times\n",
    "\n",
    "# Load the trained model\n",
    "cnn_model = load_model('abnormality_classifier_vgg16.h5')\n",
    "\n",
    "# Example usage\n",
    "video_path =  r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\C1007trimmed116-118.mp4\"\n",
    "output_dir = r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\augmentvgg_4minima\"\n",
    "abnormality_times = motion_detection(video_path, output_dir, cnn_model)\n",
    "print(\"Abnormality frames saved in the output directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 550ms/step\n",
      "Smoke detected at 0m_51.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
      "Smoke detected at 0m_51.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
      "Smoke detected at 0m_51.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
      "Smoke detected at 0m_51.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n",
      "Smoke detected at 0m_51.80s\n",
      "Abnormality 1 start time: 0 minutes 51.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 1.00 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "Smoke detected at 0m_52.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "Smoke detected at 0m_53.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "Smoke detected at 0m_53.20s\n",
      "Abnormality 1 start time: 0 minutes 52.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.60 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "Smoke detected at 0m_55.20s\n",
      "Abnormality 1 start time: 0 minutes 55.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "Smoke detected at 0m_56.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "Smoke detected at 0m_56.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n",
      "Smoke detected at 0m_56.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step\n",
      "Smoke detected at 0m_56.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step\n",
      "Smoke detected at 0m_57.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step\n",
      "Smoke detected at 0m_57.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n",
      "Smoke detected at 0m_57.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
      "Smoke detected at 0m_57.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n",
      "Smoke detected at 0m_57.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
      "Smoke detected at 0m_58.00s\n",
      "Abnormality 1 start time: 0 minutes 56.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 2.00 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
      "Smoke detected at 0m_58.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n",
      "Smoke detected at 0m_58.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
      "Smoke detected at 0m_58.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step\n",
      "Smoke detected at 0m_59.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
      "Smoke detected at 0m_59.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "Smoke detected at 0m_59.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
      "Smoke detected at 0m_59.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step\n",
      "Smoke detected at 0m_59.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
      "Smoke detected at 0m_60.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step\n",
      "Smoke detected at 1m_0.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "Smoke detected at 1m_0.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "Smoke detected at 1m_0.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step\n",
      "Smoke detected at 1m_0.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "Smoke detected at 1m_1.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
      "Smoke detected at 1m_1.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step\n",
      "Smoke detected at 1m_1.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "Smoke detected at 1m_1.60s\n",
      "Abnormality 1 start time: 0 minutes 58.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 3.40 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step\n",
      "Smoke detected at 1m_2.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "Smoke detected at 1m_2.20s\n",
      "Abnormality 1 start time: 1 minutes 2.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.40 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "Smoke detected at 1m_2.80s\n",
      "Abnormality 1 start time: 1 minutes 2.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "Smoke detected at 1m_7.20s\n",
      "Abnormality 1 start time: 1 minutes 7.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "Smoke detected at 1m_7.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "Smoke detected at 1m_7.80s\n",
      "Abnormality 1 start time: 1 minutes 7.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.40 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "Smoke detected at 1m_8.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step\n",
      "Smoke detected at 1m_9.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "Smoke detected at 1m_9.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step\n",
      "Smoke detected at 1m_9.40s\n",
      "Abnormality 1 start time: 1 minutes 8.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.80 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step\n",
      "Smoke detected at 1m_10.80s\n",
      "Abnormality 1 start time: 1 minutes 10.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
      "Smoke detected at 1m_12.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step\n",
      "Smoke detected at 1m_12.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step\n",
      "Smoke detected at 1m_12.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
      "Smoke detected at 1m_13.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n",
      "Smoke detected at 1m_13.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step\n",
      "Smoke detected at 1m_13.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step\n",
      "Smoke detected at 1m_13.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n",
      "Smoke detected at 1m_13.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n",
      "Smoke detected at 1m_14.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
      "Smoke detected at 1m_14.20s\n",
      "Abnormality 1 start time: 1 minutes 12.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 2.00 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "Smoke detected at 1m_15.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n",
      "Smoke detected at 1m_15.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n",
      "Smoke detected at 1m_15.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step\n",
      "Smoke detected at 1m_15.60s\n",
      "Abnormality 1 start time: 1 minutes 15.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.80 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step\n",
      "Smoke detected at 1m_16.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
      "Smoke detected at 1m_17.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step\n",
      "Smoke detected at 1m_17.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step\n",
      "Smoke detected at 1m_17.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
      "Smoke detected at 1m_17.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n",
      "Smoke detected at 1m_17.80s\n",
      "Abnormality 1 start time: 1 minutes 16.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 1.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n",
      "Smoke detected at 1m_18.40s\n",
      "Abnormality 1 start time: 1 minutes 18.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338ms/step\n",
      "Smoke detected at 1m_19.20s\n",
      "Abnormality 1 start time: 1 minutes 19.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "Total video time: 1 minutes 60.00 seconds\n",
      "Abnormality frames saved in the output directory.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Use the model to classify abnormalities\n",
    "def classify_abnormality(model, frame):\n",
    "    frame_resized = cv2.resize(frame, (128, 128))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    classes = ['Fire', 'Smoke']\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "# Update the process_frame function to include classification and saving frames\n",
    "def process_frame(frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, cnn_model):\n",
    "    frame_resized = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    frame_diff = cv2.absdiff(prev_frame_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = any(cv2.contourArea(contour) > min_contour_area for contour in contours)\n",
    "    if detected:\n",
    "        # Classify the abnormality\n",
    "        classification = classify_abnormality(cnn_model, frame_resized)\n",
    "        \n",
    "        abnormality_time = frame_number / original_fps\n",
    "        timestamp = f\"{int(abnormality_time // 60)}m_{abnormality_time % 60:.2f}s\"\n",
    "        output_frame_path = os.path.join(output_dir, classification, f\"{classification}_{timestamp}_frame_{frame_number}.jpg\")\n",
    "\n",
    "        os.makedirs(os.path.dirname(output_frame_path), exist_ok=True)\n",
    "        cv2.imwrite(output_frame_path, frame_resized)\n",
    "\n",
    "        print(f\"{classification} detected at {timestamp}\")\n",
    "    \n",
    "    return detected, frame_gray\n",
    "\n",
    "# Motion detection function\n",
    "def motion_detection(video_path, output_dir, cnn_model, threshold_value=25, min_contour_area=500, resize_factor=0.5, minimal_fps=5, min_abnormality_gap_minutes=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip_interval = int(round(original_fps / minimal_fps))\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame\")\n",
    "        return\n",
    "\n",
    "    prev_frame = cv2.resize(prev_frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    abnormality_times = []\n",
    "\n",
    "    frame_number = 0\n",
    "    abnormality_detected = False\n",
    "    abnormality_start_frame = None\n",
    "    abnormality_group_id = 0\n",
    "    last_abnormality_time = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pool = ThreadPool(processes=4)  # Use a thread pool with 4 threads\n",
    "\n",
    "    while True:\n",
    "        for _ in range(frame_skip_interval):\n",
    "            ret = cap.grab()  # Use grab to skip frames efficiently\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_number += 1\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.retrieve()  # Retrieve the frame after skipping\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use thread pool to process frame\n",
    "        result = pool.apply_async(process_frame, (frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir, cnn_model))\n",
    "        detected, prev_frame_gray = result.get()\n",
    "\n",
    "        if detected:\n",
    "            current_abnormality_time = frame_number / original_fps\n",
    "\n",
    "            if not abnormality_detected:\n",
    "                if last_abnormality_time is None or (current_abnormality_time - last_abnormality_time) >= (min_abnormality_gap_minutes * 60):\n",
    "                    abnormality_group_id += 1\n",
    "\n",
    "                abnormality_start_frame = frame_number\n",
    "                abnormality_detected = True\n",
    "                abnormality_times.append(frame_number)\n",
    "                last_abnormality_time = current_abnormality_time\n",
    "\n",
    "        else:\n",
    "            if abnormality_detected:\n",
    "                abnormality_end_time = frame_number / original_fps\n",
    "                abnormality_duration_seconds = abnormality_end_time - (abnormality_start_frame / original_fps)\n",
    "\n",
    "                print(f\"Abnormality {abnormality_group_id} start time: {int((abnormality_start_frame / original_fps) // 60)} minutes {(abnormality_start_frame / original_fps) % 60:.2f} seconds\")\n",
    "                print(f\"Abnormality {abnormality_group_id} duration: {int(abnormality_duration_seconds // 60)} minutes {abnormality_duration_seconds % 60:.2f} seconds\")\n",
    "\n",
    "                abnormality_detected = False\n",
    "\n",
    "    cap.release()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    total_time_seconds = frame_number / original_fps\n",
    "    print(f\"Total video time: {int(total_time_seconds // 60)} minutes {total_time_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    return abnormality_times\n",
    "\n",
    "# Load the trained model\n",
    "cnn_model = load_model('abnormality_classifier_vgg16.h5')\n",
    "\n",
    "# Example usage\n",
    "video_path =  r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\2-DMG_RSA_HTE101_D_HS_FIRE_31-33min\\output31.mp4\"\n",
    "output_dir = r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\augmentvgg_5minima\"\n",
    "abnormality_times = motion_detection(video_path, output_dir, cnn_model)\n",
    "print(\"Abnormality frames saved in the output directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46 images belonging to 2 classes.\n",
      "Found 10 images belonging to 2 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkathi\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4s/step - accuracy: 0.3859 - loss: 0.9660 - val_accuracy: 0.8000 - val_loss: 0.5785\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 795ms/step - accuracy: 0.6431 - loss: 0.8809 - val_accuracy: 0.6000 - val_loss: 0.6950\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 591ms/step - accuracy: 0.3361 - loss: 0.8700 - val_accuracy: 0.1000 - val_loss: 0.7538\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 752ms/step - accuracy: 0.4461 - loss: 0.8372 - val_accuracy: 0.2000 - val_loss: 0.7642\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.4669 - loss: 0.8076 - val_accuracy: 0.1000 - val_loss: 0.7184\n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.5249 - loss: 0.8438 - val_accuracy: 0.6000 - val_loss: 0.6928\n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 939ms/step - accuracy: 0.5892 - loss: 0.6610 - val_accuracy: 0.8000 - val_loss: 0.6492\n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.3509 - loss: 0.8278 - val_accuracy: 0.8000 - val_loss: 0.6581\n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.5859 - loss: 0.7839 - val_accuracy: 0.3000 - val_loss: 0.7060\n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 856ms/step - accuracy: 0.5435 - loss: 0.6495 - val_accuracy: 0.2000 - val_loss: 0.7370\n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.5052 - loss: 0.6958 - val_accuracy: 0.2000 - val_loss: 0.7593\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.4089 - loss: 0.8116 - val_accuracy: 0.2000 - val_loss: 0.7944\n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 766ms/step - accuracy: 0.4067 - loss: 0.8067 - val_accuracy: 0.2000 - val_loss: 0.8044\n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.5280 - loss: 0.7539 - val_accuracy: 0.2000 - val_loss: 0.8659\n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 886ms/step - accuracy: 0.4710 - loss: 0.7264 - val_accuracy: 0.2000 - val_loss: 0.8366\n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.4420 - loss: 0.7968 - val_accuracy: 0.2000 - val_loss: 0.8195\n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 758ms/step - accuracy: 0.6141 - loss: 0.6351 - val_accuracy: 0.2000 - val_loss: 0.7766\n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 745ms/step - accuracy: 0.4398 - loss: 0.7444 - val_accuracy: 0.2000 - val_loss: 0.7556\n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 767ms/step - accuracy: 0.6495 - loss: 0.6280 - val_accuracy: 0.4000 - val_loss: 0.7005\n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 921ms/step - accuracy: 0.5707 - loss: 0.6723 - val_accuracy: 0.8000 - val_loss: 0.6423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: {'Fire': 0, 'Smoke': 1}\n",
      "Number of training samples: 46\n",
      "Number of validation samples: 10\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Load and preprocess dataset with data augmentation\n",
    "def prepare_dataset(data_dir, target_size=(128, 128), batch_size=32):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255.0,\n",
    "        validation_split=0.2,\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    validation_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255.0,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Define the model using transfer learning (ResNet50)\n",
    "def create_transfer_learning_model(input_shape, num_classes):\n",
    "    base_model = ResNet50(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model with class weighting\n",
    "def train_model(model, train_generator, validation_generator, epochs=20):\n",
    "    # Extract class indices from the training generator\n",
    "    class_indices = {v: k for k, v in train_generator.class_indices.items()}\n",
    "    classes = np.array([class_indices[i] for i in train_generator.classes])\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
    "    class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        class_weight=class_weights_dict\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "data_dir = r'C:\\Users\\pkathi\\Desktop\\object detection\\tradianal approach-cv\\Dataset2-minimal'\n",
    "input_shape = (128, 128, 3)\n",
    "num_classes = 2\n",
    "\n",
    "train_gen, val_gen = prepare_dataset(data_dir, target_size=(128, 128))\n",
    "resnet_model = create_transfer_learning_model(input_shape, num_classes)\n",
    "train_model(resnet_model, train_gen, val_gen)\n",
    "\n",
    "# Save the trained model\n",
    "resnet_model.save('abnormality_classifier_resnet50.h5')\n",
    "\n",
    "# Verify the dataset preparation\n",
    "print(f\"Classes found: {train_gen.class_indices}\")\n",
    "print(f\"Number of training samples: {train_gen.samples}\")\n",
    "print(f\"Number of validation samples: {val_gen.samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Fire detected at 0m_57.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Fire detected at 0m_57.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Fire detected at 0m_58.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Fire detected at 0m_58.20s\n",
      "Abnormality 1 start time: 0 minutes 58.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Fire detected at 0m_58.60s\n",
      "Abnormality 1 start time: 0 minutes 58.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Fire detected at 1m_3.00s\n",
      "Abnormality 1 start time: 1 minutes 3.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Fire detected at 1m_3.40s\n",
      "Abnormality 1 start time: 1 minutes 3.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Fire detected at 1m_17.00s\n",
      "Abnormality 1 start time: 1 minutes 17.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Fire detected at 1m_17.40s\n",
      "Abnormality 1 start time: 1 minutes 17.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "Total video time: 1 minutes 60.00 seconds\n",
      "Abnormality detected at frame numbers: [1728, 1734, 1740, 1746, 1758, 1890, 1902, 2310, 2322]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Load the trained model\n",
    "resnet_model = load_model('abnormality_classifier_resnet50.h5')\n",
    "\n",
    "# Classify the abnormality in a frame\n",
    "def classify_abnormality(model, frame):\n",
    "    frame_resized = cv2.resize(frame, (128, 128))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    classes = ['Fire', 'Smoke']\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "# Update the process_frame function to include classification and saving frames\n",
    "def process_frame(frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir):\n",
    "    frame_resized = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    frame_diff = cv2.absdiff(prev_frame_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = any(cv2.contourArea(contour) > min_contour_area for contour in contours)\n",
    "    if detected:\n",
    "        # Classify the abnormality\n",
    "        classification = classify_abnormality(resnet_model, frame_resized)\n",
    "        \n",
    "        abnormality_time = frame_number / original_fps\n",
    "        timestamp = f\"{int(abnormality_time // 60)}m_{abnormality_time % 60:.2f}s\"\n",
    "        output_frame_path = os.path.join(output_dir, classification, f\"{classification}_{timestamp}_frame_{frame_number}.jpg\")\n",
    "\n",
    "        os.makedirs(os.path.dirname(output_frame_path), exist_ok=True)\n",
    "        cv2.imwrite(output_frame_path, frame_resized)\n",
    "\n",
    "        print(f\"{classification} detected at {timestamp}\")\n",
    "    \n",
    "    return detected, frame_gray\n",
    "\n",
    "# Motion detection function remains the same\n",
    "def motion_detection(video_path, output_dir, threshold_value=25, min_contour_area=500, resize_factor=0.5, minimal_fps=5, min_abnormality_gap_minutes=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip_interval = int(round(original_fps / minimal_fps))\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame\")\n",
    "        return\n",
    "\n",
    "    prev_frame = cv2.resize(prev_frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    abnormality_times = []\n",
    "\n",
    "    frame_number = 0\n",
    "    abnormality_detected = False\n",
    "    abnormality_start_frame = None\n",
    "    abnormality_group_id = 0\n",
    "    last_abnormality_time = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pool = ThreadPool(processes=4)  # Use a thread pool with 4 threads\n",
    "\n",
    "    while True:\n",
    "        for _ in range(frame_skip_interval):\n",
    "            ret = cap.grab()  # Use grab to skip frames efficiently\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_number += 1\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.retrieve()  # Retrieve the frame after skipping\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use thread pool to process frame\n",
    "        result = pool.apply_async(process_frame, (frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir))\n",
    "        detected, prev_frame_gray = result.get()\n",
    "\n",
    "        if detected:\n",
    "            current_abnormality_time = frame_number / original_fps\n",
    "            if last_abnormality_time is None or (current_abnormality_time - last_abnormality_time) >= (min_abnormality_gap_minutes * 60):\n",
    "                abnormality_group_id += 1\n",
    "\n",
    "            abnormality_start_frame = frame_number\n",
    "            abnormality_detected = True\n",
    "            abnormality_times.append(frame_number)\n",
    "            last_abnormality_time = current_abnormality_time\n",
    "\n",
    "        else:\n",
    "            if abnormality_detected:\n",
    "                abnormality_end_time = frame_number / original_fps\n",
    "                abnormality_duration_seconds = abnormality_end_time - (abnormality_start_frame / original_fps)\n",
    "\n",
    "                print(f\"Abnormality {abnormality_group_id} start time: {int((abnormality_start_frame / original_fps) // 60)} minutes {(abnormality_start_frame / original_fps) % 60:.2f} seconds\")\n",
    "                print(f\"Abnormality {abnormality_group_id} duration: {int(abnormality_duration_seconds // 60)} minutes {abnormality_duration_seconds % 60:.2f} seconds\")\n",
    "\n",
    "                abnormality_detected = False\n",
    "\n",
    "    cap.release()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    total_time_seconds = frame_number / original_fps\n",
    "    print(f\"Total video time: {int(total_time_seconds // 60)} minutes {total_time_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    return abnormality_times\n",
    "\n",
    "# Example usage\n",
    "video_path =  r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\C1007trimmed116-118.mp4\"\n",
    "output_dir = r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\augmentresnet_1minima\"\n",
    "abnormality_times = motion_detection(video_path, output_dir, minimal_fps=5)\n",
    "\n",
    "if abnormality_times:\n",
    "    print(\"Abnormality detected at frame numbers:\", abnormality_times)\n",
    "else:\n",
    "    print(\"No abnormality detected in the video.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Fire detected at 0m_51.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Fire detected at 0m_51.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "Fire detected at 0m_51.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "Fire detected at 0m_51.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "Fire detected at 0m_51.80s\n",
      "Abnormality 1 start time: 0 minutes 51.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "Fire detected at 0m_52.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "Fire detected at 0m_53.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "Fire detected at 0m_53.20s\n",
      "Abnormality 1 start time: 0 minutes 53.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "Fire detected at 0m_55.20s\n",
      "Abnormality 1 start time: 0 minutes 55.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Fire detected at 0m_56.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "Fire detected at 0m_56.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "Fire detected at 0m_56.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Fire detected at 0m_56.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "Fire detected at 0m_57.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Fire detected at 0m_57.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "Fire detected at 0m_57.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Fire detected at 0m_57.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Fire detected at 0m_57.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Fire detected at 0m_58.00s\n",
      "Abnormality 1 start time: 0 minutes 58.00 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Fire detected at 0m_58.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "Fire detected at 0m_58.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "Fire detected at 0m_58.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "Fire detected at 0m_59.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Fire detected at 0m_59.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Fire detected at 0m_59.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Fire detected at 0m_59.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "Fire detected at 0m_59.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Fire detected at 0m_60.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Fire detected at 1m_0.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Fire detected at 1m_0.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Fire detected at 1m_0.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Fire detected at 1m_0.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "Fire detected at 1m_1.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "Fire detected at 1m_1.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Fire detected at 1m_1.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Fire detected at 1m_1.60s\n",
      "Abnormality 1 start time: 1 minutes 1.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "Fire detected at 1m_2.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Fire detected at 1m_2.20s\n",
      "Abnormality 1 start time: 1 minutes 2.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "Fire detected at 1m_2.80s\n",
      "Abnormality 1 start time: 1 minutes 2.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "Fire detected at 1m_7.20s\n",
      "Abnormality 1 start time: 1 minutes 7.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "Fire detected at 1m_7.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "Fire detected at 1m_7.80s\n",
      "Abnormality 1 start time: 1 minutes 7.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Fire detected at 1m_8.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Fire detected at 1m_9.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "Fire detected at 1m_9.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Fire detected at 1m_9.40s\n",
      "Abnormality 1 start time: 1 minutes 9.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "Fire detected at 1m_10.80s\n",
      "Abnormality 1 start time: 1 minutes 10.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "Fire detected at 1m_12.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Fire detected at 1m_12.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Fire detected at 1m_12.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Fire detected at 1m_13.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "Fire detected at 1m_13.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "Fire detected at 1m_13.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Fire detected at 1m_13.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Fire detected at 1m_13.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "Fire detected at 1m_14.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Fire detected at 1m_14.20s\n",
      "Abnormality 1 start time: 1 minutes 14.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Fire detected at 1m_15.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Fire detected at 1m_15.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "Fire detected at 1m_15.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "Fire detected at 1m_15.60s\n",
      "Abnormality 1 start time: 1 minutes 15.60 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "Fire detected at 1m_16.80s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Fire detected at 1m_17.00s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Fire detected at 1m_17.20s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "Fire detected at 1m_17.40s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "Fire detected at 1m_17.60s\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Fire detected at 1m_17.80s\n",
      "Abnormality 1 start time: 1 minutes 17.80 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "Fire detected at 1m_18.40s\n",
      "Abnormality 1 start time: 1 minutes 18.40 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "Fire detected at 1m_19.20s\n",
      "Abnormality 1 start time: 1 minutes 19.20 seconds\n",
      "Abnormality 1 duration: 0 minutes 0.20 seconds\n",
      "Total video time: 1 minutes 60.00 seconds\n",
      "Abnormality detected at frame numbers: [1530, 1536, 1542, 1548, 1554, 1584, 1590, 1596, 1656, 1686, 1692, 1698, 1704, 1710, 1716, 1722, 1728, 1734, 1740, 1752, 1758, 1764, 1770, 1776, 1782, 1788, 1794, 1800, 1806, 1812, 1818, 1824, 1830, 1836, 1842, 1848, 1860, 1866, 1884, 2016, 2028, 2034, 2064, 2070, 2076, 2082, 2124, 2172, 2178, 2184, 2190, 2196, 2202, 2208, 2214, 2220, 2226, 2250, 2256, 2262, 2268, 2304, 2310, 2316, 2322, 2328, 2334, 2352, 2376]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Load the trained model\n",
    "resnet_model = load_model('abnormality_classifier_resnet50.h5')\n",
    "\n",
    "# Classify the abnormality in a frame\n",
    "def classify_abnormality(model, frame):\n",
    "    frame_resized = cv2.resize(frame, (128, 128))  # Resize to match model input shape\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize the image\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)  # Expand dimensions to match batch size\n",
    "    prediction = model.predict(frame_expanded)\n",
    "    classes = ['Fire', 'Smoke']\n",
    "    return classes[np.argmax(prediction)]\n",
    "\n",
    "# Update the process_frame function to include classification and saving frames\n",
    "def process_frame(frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir):\n",
    "    frame_resized = cv2.resize(frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    frame_diff = cv2.absdiff(prev_frame_gray, frame_gray)\n",
    "    _, thresh = cv2.threshold(frame_diff, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected = any(cv2.contourArea(contour) > min_contour_area for contour in contours)\n",
    "    if detected:\n",
    "        # Classify the abnormality\n",
    "        classification = classify_abnormality(resnet_model, frame_resized)\n",
    "        \n",
    "        abnormality_time = frame_number / original_fps\n",
    "        timestamp = f\"{int(abnormality_time // 60)}m_{abnormality_time % 60:.2f}s\"\n",
    "        output_frame_path = os.path.join(output_dir, classification, f\"{classification}_{timestamp}_frame_{frame_number}.jpg\")\n",
    "\n",
    "        os.makedirs(os.path.dirname(output_frame_path), exist_ok=True)\n",
    "        cv2.imwrite(output_frame_path, frame_resized)\n",
    "\n",
    "        print(f\"{classification} detected at {timestamp}\")\n",
    "    \n",
    "    return detected, frame_gray\n",
    "\n",
    "# Motion detection function remains the same\n",
    "def motion_detection(video_path, output_dir, threshold_value=25, min_contour_area=500, resize_factor=0.5, minimal_fps=5, min_abnormality_gap_minutes=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_skip_interval = int(round(original_fps / minimal_fps))\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame\")\n",
    "        return\n",
    "\n",
    "    prev_frame = cv2.resize(prev_frame, (0, 0), fx=resize_factor, fy=resize_factor)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    abnormality_times = []\n",
    "\n",
    "    frame_number = 0\n",
    "    abnormality_detected = False\n",
    "    abnormality_start_frame = None\n",
    "    abnormality_group_id = 0\n",
    "    last_abnormality_time = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pool = ThreadPool(processes=4)  # Use a thread pool with 4 threads\n",
    "\n",
    "    while True:\n",
    "        for _ in range(frame_skip_interval):\n",
    "            ret = cap.grab()  # Use grab to skip frames efficiently\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_number += 1\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.retrieve()  # Retrieve the frame after skipping\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Use thread pool to process frame\n",
    "        result = pool.apply_async(process_frame, (frame, prev_frame_gray, threshold_value, min_contour_area, original_fps, frame_number, resize_factor, output_dir))\n",
    "        detected, prev_frame_gray = result.get()\n",
    "\n",
    "        if detected:\n",
    "            current_abnormality_time = frame_number / original_fps\n",
    "            if last_abnormality_time is None or (current_abnormality_time - last_abnormality_time) >= (min_abnormality_gap_minutes * 60):\n",
    "                abnormality_group_id += 1\n",
    "\n",
    "            abnormality_start_frame = frame_number\n",
    "            abnormality_detected = True\n",
    "            abnormality_times.append(frame_number)\n",
    "            last_abnormality_time = current_abnormality_time\n",
    "\n",
    "        else:\n",
    "            if abnormality_detected:\n",
    "                abnormality_end_time = frame_number / original_fps\n",
    "                abnormality_duration_seconds = abnormality_end_time - (abnormality_start_frame / original_fps)\n",
    "\n",
    "                print(f\"Abnormality {abnormality_group_id} start time: {int((abnormality_start_frame / original_fps) // 60)} minutes {(abnormality_start_frame / original_fps) % 60:.2f} seconds\")\n",
    "                print(f\"Abnormality {abnormality_group_id} duration: {int(abnormality_duration_seconds // 60)} minutes {abnormality_duration_seconds % 60:.2f} seconds\")\n",
    "\n",
    "                abnormality_detected = False\n",
    "\n",
    "    cap.release()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    total_time_seconds = frame_number / original_fps\n",
    "    print(f\"Total video time: {int(total_time_seconds // 60)} minutes {total_time_seconds % 60:.2f} seconds\")\n",
    "\n",
    "    return abnormality_times\n",
    "\n",
    "# Example usage\n",
    "video_path =  r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\2-DMG_RSA_HTE101_D_HS_FIRE_31-33min\\output31.mp4\"\n",
    "output_dir = r\"C:\\Users\\pkathi\\Desktop\\my work\\computervision-video\\augmentresnet_2minima\"\n",
    "abnormality_times = motion_detection(video_path, output_dir, minimal_fps=5)\n",
    "\n",
    "if abnormality_times:\n",
    "    print(\"Abnormality detected at frame numbers:\", abnormality_times)\n",
    "else:\n",
    "    print(\"No abnormality detected in the video.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
